{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-40z1ExjOuN3"
      },
      "source": [
        "# CNN-EDAIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1kC60Uneq7k",
        "outputId": "a006b9df-ce8a-44c6-d274-d63c5dc53196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 12s 51ms/step - loss: 0.4809 - accuracy: 0.8111 - val_loss: 0.4753 - val_accuracy: 0.8105\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.3675 - accuracy: 0.8476 - val_loss: 0.5158 - val_accuracy: 0.7640\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 0.2557 - accuracy: 0.8996 - val_loss: 0.5365 - val_accuracy: 0.7703\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 7s 35ms/step - loss: 0.1924 - accuracy: 0.9263 - val_loss: 0.5452 - val_accuracy: 0.7740\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 0.1523 - accuracy: 0.9442 - val_loss: 0.6323 - val_accuracy: 0.7667\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 7s 36ms/step - loss: 0.1308 - accuracy: 0.9483 - val_loss: 0.6415 - val_accuracy: 0.7871\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 0.1143 - accuracy: 0.9576 - val_loss: 0.7043 - val_accuracy: 0.7941\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 8s 40ms/step - loss: 0.0989 - accuracy: 0.9635 - val_loss: 0.7096 - val_accuracy: 0.7700\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 8s 40ms/step - loss: 0.0901 - accuracy: 0.9672 - val_loss: 0.7477 - val_accuracy: 0.7703\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0806 - accuracy: 0.9710 - val_loss: 0.7321 - val_accuracy: 0.7774\n",
            "77/77 [==============================] - 1s 7ms/step\n",
            "Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.93      0.85      1897\n",
            "           1       0.20      0.06      0.09       549\n",
            "\n",
            "    accuracy                           0.74      2446\n",
            "   macro avg       0.49      0.50      0.47      2446\n",
            "weighted avg       0.64      0.74      0.68      2446\n",
            "\n",
            "Accuracy for Testing Set: 0.7371\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def pad_features_to_square(X):\n",
        "    num_features = X.shape[1]\n",
        "    square_size = int(np.ceil(np.sqrt(num_features)))\n",
        "    num_to_pad = square_size**2 - num_features\n",
        "    padded_features = np.pad(X, ((0, 0), (0, num_to_pad)), mode='constant')\n",
        "    return padded_features\n",
        "\n",
        "def prepare_data_for_cnn(features, size):\n",
        "    padded_features = pad_features_to_square(features)\n",
        "    reshaped_features = padded_features.reshape((features.shape[0], size, size, 1))\n",
        "    return reshaped_features\n",
        "\n",
        "def create_model():\n",
        "    train_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv')\n",
        "    validation_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/validation_labels.csv')\n",
        "    test_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "    train_data.dropna(inplace=True)\n",
        "    validation_data.dropna(inplace=True)\n",
        "    test_data.dropna(inplace=True)\n",
        "\n",
        "    X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "    y_train = train_data['Class'].values\n",
        "    X_validation = validation_data.drop(columns=['Class', 'name']).values\n",
        "    y_validation = validation_data['Class'].values\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "\n",
        "    scaler = StandardScaler().fit(np.vstack((X_train, X_validation, X_test)))\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_validation_scaled = scaler.transform(X_validation)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    size = int(np.sqrt(pad_features_to_square(X_train_scaled).shape[1]))\n",
        "\n",
        "    X_train_cnn = prepare_data_for_cnn(X_train_scaled, size)\n",
        "    X_validation_cnn = prepare_data_for_cnn(X_validation_scaled, size)\n",
        "    X_test_cnn = prepare_data_for_cnn(X_test_scaled, size)\n",
        "\n",
        "    inputs = Input(shape=(size, size, 1))\n",
        "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train_cnn, y_train, epochs=10, batch_size=64, validation_data=(X_validation_cnn, y_validation))\n",
        "\n",
        "    y_pred = (model.predict(X_test_cnn) > 0.5).astype(int)\n",
        "    print(\"Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for Testing Set: {accuracy:.4f}\")\n",
        "\n",
        "    return model, accuracy\n",
        "\n",
        "# Call the function to create and train the model\n",
        "trained_model, test_accuracy = create_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQkEherBjowS"
      },
      "source": [
        "# **1D-CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR_tBkFSjnwm",
        "outputId": "0cb58a16-2734-4cd1-cd8d-9e1d33a775ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 19s 78ms/step - loss: 0.3201 - accuracy: 0.8661 - val_loss: 0.6138 - val_accuracy: 0.7720\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 14s 70ms/step - loss: 0.1235 - accuracy: 0.9526 - val_loss: 0.8884 - val_accuracy: 0.7780\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 14s 72ms/step - loss: 0.0715 - accuracy: 0.9741 - val_loss: 0.9786 - val_accuracy: 0.7844\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 14s 70ms/step - loss: 0.0541 - accuracy: 0.9801 - val_loss: 1.2947 - val_accuracy: 0.7921\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 14s 73ms/step - loss: 0.0428 - accuracy: 0.9838 - val_loss: 1.4049 - val_accuracy: 0.7968\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 15s 75ms/step - loss: 0.0318 - accuracy: 0.9889 - val_loss: 1.4990 - val_accuracy: 0.7864\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 14s 70ms/step - loss: 0.0267 - accuracy: 0.9905 - val_loss: 1.4713 - val_accuracy: 0.7938\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 0.0257 - accuracy: 0.9911 - val_loss: 1.3441 - val_accuracy: 0.7854\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 0.0236 - accuracy: 0.9914 - val_loss: 1.8775 - val_accuracy: 0.7991\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 12s 63ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 1.7188 - val_accuracy: 0.7918\n",
            "77/77 [==============================] - 1s 8ms/step\n",
            "Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.98      0.86      1897\n",
            "           1       0.24      0.03      0.05       549\n",
            "\n",
            "    accuracy                           0.76      2446\n",
            "   macro avg       0.51      0.50      0.46      2446\n",
            "weighted avg       0.66      0.76      0.68      2446\n",
            "\n",
            "Accuracy for Testing Set: 0.7633\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def prepare_data_for_cnn(features):\n",
        "    # Add a new axis for channels\n",
        "    reshaped_features = np.expand_dims(features, axis=-1)\n",
        "    return reshaped_features\n",
        "\n",
        "def create_model():\n",
        "    train_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv', header=0, index_col=0)\n",
        "    validation_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/validation_labels.csv', header=0, index_col=0)\n",
        "    test_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv', header=0, index_col=0)\n",
        "\n",
        "    train_data.dropna(inplace=True)\n",
        "    validation_data.dropna(inplace=True)\n",
        "    test_data.dropna(inplace=True)\n",
        "\n",
        "    # Select only the columns from the second column onwards as features\n",
        "    X_train = train_data.iloc[:, 1:].values\n",
        "    y_train = train_data['Class'].values\n",
        "    X_validation = validation_data.iloc[:, 1:].values\n",
        "    y_validation = validation_data['Class'].values\n",
        "    X_test = test_data.iloc[:, 1:].values\n",
        "    y_test = test_data['Class'].values\n",
        "\n",
        "    scaler = StandardScaler().fit(np.vstack((X_train, X_validation, X_test)))\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_validation_scaled = scaler.transform(X_validation)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_cnn = prepare_data_for_cnn(X_train_scaled)\n",
        "    X_validation_cnn = prepare_data_for_cnn(X_validation_scaled)\n",
        "    X_test_cnn = prepare_data_for_cnn(X_test_scaled)\n",
        "\n",
        "    inputs = Input(shape=(X_train_cnn.shape[1], 1))  # Input shape for 1D CNN\n",
        "    x = Conv1D(32, kernel_size=3, activation='relu')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train_cnn, y_train, epochs=10, batch_size=64, validation_data=(X_validation_cnn, y_validation))\n",
        "\n",
        "    y_pred = (model.predict(X_test_cnn) > 0.5).astype(int)\n",
        "    print(\"Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for Testing Set: {accuracy:.4f}\")\n",
        "\n",
        "    return model, accuracy\n",
        "\n",
        "# Call the function to create and train the model\n",
        "trained_model, test_accuracy = create_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XMZ0mZ4w1Ee"
      },
      "source": [
        "# **save model 1D**\n",
        "# latest_CNN.h5 is 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s07tYXgzw1WL",
        "outputId": "25ba9760-799f-432c-89c5-0978af9efe5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-a7ac0c393057>:4: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(trained_model, 'latest_CNN.h5')\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Save the model\n",
        "save_model(trained_model, 'latest_CNN.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLA4YCtRln1"
      },
      "source": [
        "# Saving the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZScwFwCtRmp_"
      },
      "outputs": [],
      "source": [
        "# After training\n",
        "import tensorflow as tf\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/CNN_2-EDAIC.h5')  # Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe9h8squnZte"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb1HQJzYSd6t"
      },
      "source": [
        "# Load and Test on EDAIC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjIGhqNySeKU",
        "outputId": "c9b291f4-ab3f-428a-8932-070aa393050b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 1s 18ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.7522\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\\\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/latest_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bawkEUzUgf5"
      },
      "source": [
        "# Test on Android"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYaz5W2VUfxE",
        "outputId": "0f251248-f813-400b-cf54-10775ab87751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "47/47 [==============================] - 1s 18ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.4323\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/latest_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOay8_C3U0PN"
      },
      "source": [
        "# **Load and Fine-tune on Android**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkKKQujrVtwK",
        "outputId": "7887843b-0cb4-4246-cab9-3962dabf9e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "85/85 [==============================] - 6s 48ms/step - loss: 1.6660 - accuracy: 0.5516 - val_loss: 0.8329 - val_accuracy: 0.5712\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 4s 50ms/step - loss: 0.7851 - accuracy: 0.5327 - val_loss: 0.6742 - val_accuracy: 0.6083\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 3s 37ms/step - loss: 0.6974 - accuracy: 0.5535 - val_loss: 0.6410 - val_accuracy: 0.6380\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.6567 - accuracy: 0.5805 - val_loss: 0.6170 - val_accuracy: 0.6543\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 4s 45ms/step - loss: 0.6334 - accuracy: 0.6121 - val_loss: 0.5956 - val_accuracy: 0.6780\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 4s 50ms/step - loss: 0.6168 - accuracy: 0.6258 - val_loss: 0.5747 - val_accuracy: 0.7047\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 4s 45ms/step - loss: 0.5923 - accuracy: 0.6689 - val_loss: 0.5529 - val_accuracy: 0.7389\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.5585 - accuracy: 0.6860 - val_loss: 0.5251 - val_accuracy: 0.7433\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 5s 62ms/step - loss: 0.5415 - accuracy: 0.7086 - val_loss: 0.4962 - val_accuracy: 0.7730\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 4s 43ms/step - loss: 0.4944 - accuracy: 0.7546 - val_loss: 0.4688 - val_accuracy: 0.7878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the training and validation dataset\n",
        "interview_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv')\n",
        "\n",
        "# Prepare the dataset by dropping the 'name' column and separating features and labels\n",
        "X = interview_df.drop(columns=['name', 'Class']).values\n",
        "y = interview_df['Class'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/latest_CNN.h5')\n",
        "\n",
        "# Compile the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fine-tune the model on the new dataset\n",
        "model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=10, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvgirJv5Xad1"
      },
      "source": [
        "# Test on Android"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YfC1ejfXcGi",
        "outputId": "2e17a62a-4051-4b02-8b4a-d9f7dcc26c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 1s 18ms/step\n",
            "Test Accuracy: 0.6184122748498999\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the test dataset\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "\n",
        "# Prepare the test dataset by dropping the 'name' column and separating features and labels\n",
        "X_test = test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = test_df['Class'].values\n",
        "\n",
        "# Scale the features of the test set using the same scaler as used for training\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = load_model('/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5')\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = fine_tuned_model.predict(X_test_scaled)\n",
        "# Convert probabilities to class labels based on a threshold (0.5 for binary classification)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, predicted_classes)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUjDfE29XcPG"
      },
      "source": [
        "# Test on EDAIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHm3OmZhXdzo",
        "outputId": "8dde7df2-b3b5-42d6-a76a-8c73e5850ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 1s 15ms/step\n",
            "Test Accuracy: 0.6369582992641046\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the test dataset\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "# Prepare the test dataset by dropping the 'name' column and separating features and labels\n",
        "X_test = test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = test_df['Class'].values\n",
        "\n",
        "# Scale the features of the test set using the same scaler as used for training\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = load_model('/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5')\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = fine_tuned_model.predict(X_test_scaled)\n",
        "# Convert probabilities to class labels based on a threshold (0.5 for binary classification)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, predicted_classes)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5juUCWQCbTdt"
      },
      "source": [
        "# CSV Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3MJ1XTdbVpf",
        "outputId": "d4ad4bce-b4f1-4799-d282-8729d3dee72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns:\n",
            "['name', 'Class', 'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213', 'feature_214', 'feature_215', 'feature_216', 'feature_217', 'feature_218', 'feature_219', 'feature_220', 'feature_221', 'feature_222', 'feature_223', 'feature_224', 'feature_225', 'feature_226', 'feature_227', 'feature_228', 'feature_229', 'feature_230', 'feature_231', 'feature_232', 'feature_233', 'feature_234', 'feature_235', 'feature_236', 'feature_237', 'feature_238', 'feature_239', 'feature_240', 'feature_241', 'feature_242', 'feature_243', 'feature_244', 'feature_245', 'feature_246', 'feature_247', 'feature_248', 'feature_249', 'feature_250', 'feature_251', 'feature_252', 'feature_253', 'feature_254', 'feature_255', 'feature_256', 'feature_257', 'feature_258', 'feature_259', 'feature_260', 'feature_261', 'feature_262', 'feature_263', 'feature_264', 'feature_265', 'feature_266', 'feature_267', 'feature_268', 'feature_269', 'feature_270', 'feature_271', 'feature_272', 'feature_273', 'feature_274', 'feature_275', 'feature_276', 'feature_277', 'feature_278', 'feature_279', 'feature_280', 'feature_281', 'feature_282', 'feature_283', 'feature_284', 'feature_285', 'feature_286', 'feature_287', 'feature_288', 'feature_289', 'feature_290', 'feature_291', 'feature_292', 'feature_293', 'feature_294', 'feature_295', 'feature_296', 'feature_297', 'feature_298', 'feature_299', 'feature_300', 'feature_301', 'feature_302', 'feature_303', 'feature_304', 'feature_305', 'feature_306', 'feature_307', 'feature_308', 'feature_309', 'feature_310', 'feature_311', 'feature_312', 'feature_313', 'feature_314', 'feature_315', 'feature_316', 'feature_317', 'feature_318', 'feature_319', 'feature_320', 'feature_321', 'feature_322', 'feature_323', 'feature_324', 'feature_325', 'feature_326', 'feature_327', 'feature_328', 'feature_329', 'feature_330', 'feature_331', 'feature_332', 'feature_333', 'feature_334', 'feature_335', 'feature_336', 'feature_337', 'feature_338', 'feature_339', 'feature_340', 'feature_341', 'feature_342', 'feature_343', 'feature_344', 'feature_345', 'feature_346', 'feature_347', 'feature_348', 'feature_349', 'feature_350', 'feature_351', 'feature_352', 'feature_353', 'feature_354', 'feature_355', 'feature_356', 'feature_357', 'feature_358', 'feature_359', 'feature_360', 'feature_361', 'feature_362', 'feature_363', 'feature_364', 'feature_365', 'feature_366', 'feature_367', 'feature_368', 'feature_369', 'feature_370', 'feature_371', 'feature_372', 'feature_373', 'feature_374', 'feature_375', 'feature_376', 'feature_377', 'feature_378', 'feature_379', 'feature_380', 'feature_381', 'feature_382', 'feature_383', 'feature_384', 'feature_385', 'feature_386', 'feature_387', 'feature_388', 'feature_389', 'feature_390', 'feature_391', 'feature_392', 'feature_393', 'feature_394', 'feature_395', 'feature_396', 'feature_397', 'feature_398', 'feature_399', 'feature_400', 'feature_401', 'feature_402', 'feature_403', 'feature_404', 'feature_405', 'feature_406', 'feature_407', 'feature_408', 'feature_409', 'feature_410', 'feature_411', 'feature_412', 'feature_413', 'feature_414', 'feature_415', 'feature_416', 'feature_417', 'feature_418', 'feature_419', 'feature_420', 'feature_421', 'feature_422', 'feature_423', 'feature_424', 'feature_425', 'feature_426', 'feature_427', 'feature_428', 'feature_429', 'feature_430', 'feature_431', 'feature_432', 'feature_433', 'feature_434', 'feature_435', 'feature_436', 'feature_437', 'feature_438', 'feature_439', 'feature_440', 'feature_441', 'feature_442', 'feature_443', 'feature_444', 'feature_445', 'feature_446', 'feature_447', 'feature_448', 'feature_449', 'feature_450', 'feature_451', 'feature_452', 'feature_453', 'feature_454', 'feature_455', 'feature_456', 'feature_457', 'feature_458', 'feature_459', 'feature_460', 'feature_461', 'feature_462', 'feature_463', 'feature_464', 'feature_465', 'feature_466', 'feature_467', 'feature_468', 'feature_469', 'feature_470', 'feature_471', 'feature_472', 'feature_473', 'feature_474', 'feature_475', 'feature_476', 'feature_477', 'feature_478', 'feature_479', 'feature_480', 'feature_481', 'feature_482', 'feature_483', 'feature_484', 'feature_485', 'feature_486', 'feature_487', 'feature_488', 'feature_489', 'feature_490', 'feature_491', 'feature_492', 'feature_493', 'feature_494', 'feature_495', 'feature_496', 'feature_497', 'feature_498', 'feature_499', 'feature_500', 'feature_501', 'feature_502', 'feature_503', 'feature_504', 'feature_505', 'feature_506', 'feature_507', 'feature_508', 'feature_509', 'feature_510', 'feature_511']\n",
            "\n",
            "Data Types:\n",
            "name            object\n",
            "Class          float64\n",
            "feature_0      float64\n",
            "feature_1      float64\n",
            "feature_2      float64\n",
            "                ...   \n",
            "feature_507    float64\n",
            "feature_508    float64\n",
            "feature_509    float64\n",
            "feature_510    float64\n",
            "feature_511    float64\n",
            "Length: 514, dtype: object\n",
            "\n",
            "Preview (first 5 rows):\n",
            "               name  Class  feature_0  feature_1  feature_2  feature_3  \\\n",
            "0   302_AUDIO_0.wav    0.0 -23.331116  -3.194857  14.467054   8.521894   \n",
            "1   302_AUDIO_1.wav    0.0 -17.767897   2.149175   8.194693   8.194651   \n",
            "2  302_AUDIO_10.wav    0.0 -20.015242  15.377477  10.758303  12.918330   \n",
            "3  302_AUDIO_11.wav    0.0 -21.528112  15.833344   8.891406  10.847867   \n",
            "4  302_AUDIO_12.wav    0.0 -23.101309  14.002757   9.993515  10.308297   \n",
            "\n",
            "   feature_4  feature_5  feature_6  feature_7  ...  feature_502  feature_503  \\\n",
            "0   2.855842   2.448179 -20.945894  -5.823390  ...   -19.878117     9.811157   \n",
            "1   3.920149   3.560336 -18.974482   2.312250  ...   -15.542802    10.029692   \n",
            "2  -2.422351   3.420472 -18.040806   1.215532  ...   -21.035236    14.472557   \n",
            "3   2.757204   5.584030 -22.892935   5.892217  ...   -23.067982    12.327912   \n",
            "4   0.696687   3.116205 -19.739790   7.510989  ...   -27.427729    12.648687   \n",
            "\n",
            "   feature_504  feature_505  feature_506  feature_507  feature_508  \\\n",
            "0    11.584371    -2.996328    -6.866749    10.263261     8.573358   \n",
            "1     9.301889    -1.671853     1.379695     9.192435     6.095900   \n",
            "2     9.419411    -3.498692     1.338385    11.500857    14.127392   \n",
            "3     4.246151    -3.064113     6.042052     9.165711    16.361263   \n",
            "4     5.889781    -3.043681     4.174570     9.108904    11.026467   \n",
            "\n",
            "   feature_509  feature_510  feature_511  \n",
            "0     7.451781     9.449115   -16.714563  \n",
            "1     6.742350     5.617034   -13.074841  \n",
            "2   -12.540409    10.330028   -16.549768  \n",
            "3   -12.010640    10.497300   -17.941319  \n",
            "4   -12.769280    10.371862   -18.186577  \n",
            "\n",
            "[5 rows x 514 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def csv_details(file_path):\n",
        "    # Load CSV file into a DataFrame\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "        return\n",
        "\n",
        "    # Display basic details\n",
        "    print(\"Columns:\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nPreview (first 5 rows):\")\n",
        "    print(df.head())\n",
        "\n",
        "# Example usage:\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv'  # Replace this with the path to your CSV file\n",
        "csv_details(csv_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5juUCWQCbTdt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
