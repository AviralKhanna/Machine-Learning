{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqEAG4F7faroWJBBZm5z6m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"is-68uRNQsE1"},"outputs":[],"source":["pip install git+https://github.com/huggingface/transformers.git@main accelerate"]},{"cell_type":"code","source":["!pip install bitsandbytes"],"metadata":{"id":"hTZuIfZJRMFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"JMXBPrtPRN2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install peft"],"metadata":{"id":"rlPPVdOXRQgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install trl"],"metadata":{"id":"2rvhZyS_RTH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""],"metadata":{"id":"GeloS0VURXXh","executionInfo":{"status":"ok","timestamp":1704536178592,"user_tz":-330,"elapsed":457,"user":{"displayName":"girish girish","userId":"16813817461950920468"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"RVkSoNs0Rgis","executionInfo":{"status":"ok","timestamp":1704536183640,"user_tz":-330,"elapsed":3,"user":{"displayName":"girish girish","userId":"16813817461950920468"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import Dataset\n","from peft import LoraConfig, PeftConfig\n","from trl import SFTTrainer\n","from transformers import (AutoModelForCausalLM,\n","                          AutoTokenizer,\n","                          BitsAndBytesConfig,\n","                          TrainingArguments,\n","                          pipeline,\n","                          logging)\n","from sklearn.metrics import (accuracy_score,\n","                             classification_report,\n","                             confusion_matrix)\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSxcSGNIRhOf","executionInfo":{"status":"ok","timestamp":1704536211924,"user_tz":-330,"elapsed":23233,"user":{"displayName":"girish girish","userId":"16813817461950920468"}},"outputId":"3b22b48c-f738-4000-8f49-3ad044ea4ace"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"]}]},{"cell_type":"code","source":["print(f\"pytorch version {torch.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaVG7_b-Rjx5","executionInfo":{"status":"ok","timestamp":1704536211925,"user_tz":-330,"elapsed":3,"user":{"displayName":"girish girish","userId":"16813817461950920468"}},"outputId":"8f7e7e60-d308-49ad-fc50-8199b8dd53f5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["pytorch version 2.1.0+cu121\n"]}]},{"cell_type":"code","source":["model_name = \"codellama/CodeLlama-7b-hf\"\n","\n","compute_dtype = getattr(torch, \"float16\")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","    quantization_config=bnb_config,\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                                          trust_remote_code=True,\n","                                         )\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"5Le3SE7zRpUE","executionInfo":{"status":"error","timestamp":1704536300568,"user_tz":-330,"elapsed":3896,"user":{"displayName":"girish girish","userId":"16813817461950920468"}},"outputId":"3b1f7fd8-fe9d-4b60-9fbf-508a5b904488"},"execution_count":12,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"No GPU found. A GPU is needed for quantization.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-574e5bddf334>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2930\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 raise ImportError(\n","\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."]}]},{"cell_type":"code","source":[],"metadata":{"id":"X9X45EnnRrbN"},"execution_count":null,"outputs":[]}]}