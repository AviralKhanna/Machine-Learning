{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H5iDsm6BbyRo",
        "5Kcs4bcPeYh7",
        "tOJOmIXJSzGX",
        "1yPdhJ0RTCST",
        "AF5EjxMJTMvn",
        "yQ8l1uV7TZXE",
        "r6ksPxGqTpYV",
        "KRvkyiQOVQM4",
        "WMxuuvhdatsk",
        "x7Pd1uXY8-sP",
        "xSUN-jO1ifXu",
        "_NSQ3Xty1kXu"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e799565ab6c4ffa8605de93fe7a5213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab75738523c24229802a729ffd1d409a",
              "IPY_MODEL_c6e8444a0fbc41fba49e64d640b3bbd1",
              "IPY_MODEL_c54b0ee8da1b48148a668bf8686c7434"
            ],
            "layout": "IPY_MODEL_feffe3ca3396475598916362eea8439a"
          }
        },
        "ab75738523c24229802a729ffd1d409a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f326d871e6c34c90ae200cfa8430ea6d",
            "placeholder": "​",
            "style": "IPY_MODEL_4c73b8db04a54ff8baa616025bc35e67",
            "value": "Dl Completed...: 100%"
          }
        },
        "c6e8444a0fbc41fba49e64d640b3bbd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83bf83ced001496e98c259954017edc1",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0523566155ef47ff9766d587e3ed45b0",
            "value": 5
          }
        },
        "c54b0ee8da1b48148a668bf8686c7434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_670275126ab0496eab80a69ff4396a24",
            "placeholder": "​",
            "style": "IPY_MODEL_8458db3e39dd4888b90eba7c7029b7dc",
            "value": " 5/5 [00:00&lt;00:00,  8.85 file/s]"
          }
        },
        "feffe3ca3396475598916362eea8439a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f326d871e6c34c90ae200cfa8430ea6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c73b8db04a54ff8baa616025bc35e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83bf83ced001496e98c259954017edc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0523566155ef47ff9766d587e3ed45b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "670275126ab0496eab80a69ff4396a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8458db3e39dd4888b90eba7c7029b7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **intro**"
      ],
      "metadata": {
        "id": "H5iDsm6BbyRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the pre-trained CNN model\n",
        "model = load_model('/content/drive/MyDrive/Depression/Model/finetune_on_android.h5')\n",
        "\n",
        "# Define function to load CSV data and preprocess\n",
        "def load_data(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    X = df.drop(['name', 'Class'], axis=1)  # Assuming 'name' and 'Class' are not features\n",
        "    y = df['Class']\n",
        "    # Encode class labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "# Paths to CSV files for training and testing data of both datasets\n",
        "interview_train_csv = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "interview_test_csv = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "reading_train_csv = '/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv'\n",
        "reading_test_csv = '/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv'\n",
        "\n",
        "# Load data for Interview dataset\n",
        "X_interview_train, y_interview_train = load_data(interview_train_csv)\n",
        "X_interview_test, y_interview_test = load_data(interview_test_csv)\n",
        "\n",
        "# Load data for Reading dataset\n",
        "X_reading_train, y_reading_train = load_data(reading_train_csv)\n",
        "X_reading_test, y_reading_test = load_data(reading_test_csv)\n",
        "\n",
        "# Compile the model (if necessary)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks (optional but recommended, e.g., for saving best model during training)\n",
        "checkpoint_callback = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "# Evaluate the model on Interview train data before fine-tuning\n",
        "interview_train_accuracy_before = accuracy_score(y_interview_train, model.predict_classes(X_interview_train))\n",
        "print(f'Accuracy on Interview train data before fine-tuning: {interview_train_accuracy_before}')\n",
        "\n",
        "# Train the model on Interview dataset\n",
        "model.fit(X_interview_train, y_interview_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint_callback])\n",
        "\n",
        "# Evaluate the model on Interview train data after fine-tuning\n",
        "interview_train_accuracy_after = accuracy_score(y_interview_train, model.predict_classes(X_interview_train))\n",
        "print(f'Accuracy on Interview train data after fine-tuning: {interview_train_accuracy_after}')\n",
        "\n",
        "# Print message if catastrophic forgetting occurred\n",
        "if interview_train_accuracy_after < interview_train_accuracy_before:\n",
        "    print(\"Catastrophic forgetting occurs here.\")\n",
        "\n",
        "# Load the best model saved during training (optional, if using callbacks)\n",
        "best_model = load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on Interview test data\n",
        "interview_test_accuracy = accuracy_score(y_interview_test, best_model.predict_classes(X_interview_test))\n",
        "print(f'Accuracy on Interview test data: {interview_test_accuracy}')\n",
        "\n",
        "# Evaluate the model on Reading test data\n",
        "reading_test_accuracy = accuracy_score(y_reading_test, best_model.predict_classes(X_reading_test))\n",
        "print(f'Accuracy on Reading test data: {reading_test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "td3D2TTubyWz",
        "outputId": "4d29ad6a-69b6-4300-d9ca-b283beecf91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Functional' object has no attribute 'predict_classes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ded4b29cd171>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Evaluate the model on Interview train data before fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0minterview_train_accuracy_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_interview_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_interview_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy on Interview train data before fine-tuning: {interview_train_accuracy_before}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'predict_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def csv_details(file_path):\n",
        "    # Load CSV file into a DataFrame\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "        return\n",
        "\n",
        "    # Display basic details\n",
        "    print(\"Columns:\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nPreview (first 5 rows):\")\n",
        "    print(df.head())\n",
        "\n",
        "# Example usage:\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'  # Replace this with the path to your CSV file\n",
        "csv_details(csv_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVLVVKJsckMN",
        "outputId": "839b411b-0adc-4444-c915-d071c5b23c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "['name', 'Class', 'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213', 'feature_214', 'feature_215', 'feature_216', 'feature_217', 'feature_218', 'feature_219', 'feature_220', 'feature_221', 'feature_222', 'feature_223', 'feature_224', 'feature_225', 'feature_226', 'feature_227', 'feature_228', 'feature_229', 'feature_230', 'feature_231', 'feature_232', 'feature_233', 'feature_234', 'feature_235', 'feature_236', 'feature_237', 'feature_238', 'feature_239', 'feature_240', 'feature_241', 'feature_242', 'feature_243', 'feature_244', 'feature_245', 'feature_246', 'feature_247', 'feature_248', 'feature_249', 'feature_250', 'feature_251', 'feature_252', 'feature_253', 'feature_254', 'feature_255', 'feature_256', 'feature_257', 'feature_258', 'feature_259', 'feature_260', 'feature_261', 'feature_262', 'feature_263', 'feature_264', 'feature_265', 'feature_266', 'feature_267', 'feature_268', 'feature_269', 'feature_270', 'feature_271', 'feature_272', 'feature_273', 'feature_274', 'feature_275', 'feature_276', 'feature_277', 'feature_278', 'feature_279', 'feature_280', 'feature_281', 'feature_282', 'feature_283', 'feature_284', 'feature_285', 'feature_286', 'feature_287', 'feature_288', 'feature_289', 'feature_290', 'feature_291', 'feature_292', 'feature_293', 'feature_294', 'feature_295', 'feature_296', 'feature_297', 'feature_298', 'feature_299', 'feature_300', 'feature_301', 'feature_302', 'feature_303', 'feature_304', 'feature_305', 'feature_306', 'feature_307', 'feature_308', 'feature_309', 'feature_310', 'feature_311', 'feature_312', 'feature_313', 'feature_314', 'feature_315', 'feature_316', 'feature_317', 'feature_318', 'feature_319', 'feature_320', 'feature_321', 'feature_322', 'feature_323', 'feature_324', 'feature_325', 'feature_326', 'feature_327', 'feature_328', 'feature_329', 'feature_330', 'feature_331', 'feature_332', 'feature_333', 'feature_334', 'feature_335', 'feature_336', 'feature_337', 'feature_338', 'feature_339', 'feature_340', 'feature_341', 'feature_342', 'feature_343', 'feature_344', 'feature_345', 'feature_346', 'feature_347', 'feature_348', 'feature_349', 'feature_350', 'feature_351', 'feature_352', 'feature_353', 'feature_354', 'feature_355', 'feature_356', 'feature_357', 'feature_358', 'feature_359', 'feature_360', 'feature_361', 'feature_362', 'feature_363', 'feature_364', 'feature_365', 'feature_366', 'feature_367', 'feature_368', 'feature_369', 'feature_370', 'feature_371', 'feature_372', 'feature_373', 'feature_374', 'feature_375', 'feature_376', 'feature_377', 'feature_378', 'feature_379', 'feature_380', 'feature_381', 'feature_382', 'feature_383', 'feature_384', 'feature_385', 'feature_386', 'feature_387', 'feature_388', 'feature_389', 'feature_390', 'feature_391', 'feature_392', 'feature_393', 'feature_394', 'feature_395', 'feature_396', 'feature_397', 'feature_398', 'feature_399', 'feature_400', 'feature_401', 'feature_402', 'feature_403', 'feature_404', 'feature_405', 'feature_406', 'feature_407', 'feature_408', 'feature_409', 'feature_410', 'feature_411', 'feature_412', 'feature_413', 'feature_414', 'feature_415', 'feature_416', 'feature_417', 'feature_418', 'feature_419', 'feature_420', 'feature_421', 'feature_422', 'feature_423', 'feature_424', 'feature_425', 'feature_426', 'feature_427', 'feature_428', 'feature_429', 'feature_430', 'feature_431', 'feature_432', 'feature_433', 'feature_434', 'feature_435', 'feature_436', 'feature_437', 'feature_438', 'feature_439', 'feature_440', 'feature_441', 'feature_442', 'feature_443', 'feature_444', 'feature_445', 'feature_446', 'feature_447', 'feature_448', 'feature_449', 'feature_450', 'feature_451', 'feature_452', 'feature_453', 'feature_454', 'feature_455', 'feature_456', 'feature_457', 'feature_458', 'feature_459', 'feature_460', 'feature_461', 'feature_462', 'feature_463', 'feature_464', 'feature_465', 'feature_466', 'feature_467', 'feature_468', 'feature_469', 'feature_470', 'feature_471', 'feature_472', 'feature_473', 'feature_474', 'feature_475', 'feature_476', 'feature_477', 'feature_478', 'feature_479', 'feature_480', 'feature_481', 'feature_482', 'feature_483', 'feature_484', 'feature_485', 'feature_486', 'feature_487', 'feature_488', 'feature_489', 'feature_490', 'feature_491', 'feature_492', 'feature_493', 'feature_494', 'feature_495', 'feature_496', 'feature_497', 'feature_498', 'feature_499', 'feature_500', 'feature_501', 'feature_502', 'feature_503', 'feature_504', 'feature_505', 'feature_506', 'feature_507', 'feature_508', 'feature_509', 'feature_510', 'feature_511']\n",
            "\n",
            "Data Types:\n",
            "name            object\n",
            "Class            int64\n",
            "feature_0      float64\n",
            "feature_1      float64\n",
            "feature_2      float64\n",
            "                ...   \n",
            "feature_507    float64\n",
            "feature_508    float64\n",
            "feature_509    float64\n",
            "feature_510    float64\n",
            "feature_511    float64\n",
            "Length: 514, dtype: object\n",
            "\n",
            "Preview (first 5 rows):\n",
            "                           name  Class  feature_0  feature_1  feature_2  \\\n",
            "0     Interview_depressed_1.wav      1 -20.201351 -12.129687  13.558279   \n",
            "1    Interview_depressed_10.wav      1 -22.269053  -2.444332  14.696182   \n",
            "2  Interview_depressed_1000.wav      1 -22.223230  -5.802803  13.984446   \n",
            "3  Interview_depressed_1001.wav      1 -22.192949   6.002230  11.436386   \n",
            "4  Interview_depressed_1002.wav      1 -19.286789  -2.906753  13.882933   \n",
            "\n",
            "   feature_3  feature_4  feature_5  feature_6  feature_7  ...  feature_502  \\\n",
            "0  13.090107  -4.606802 -15.516593 -19.373817  -8.865891  ...   -21.749886   \n",
            "1  10.069239  -4.511178  -1.556137 -23.446640 -11.044399  ...   -23.884607   \n",
            "2   7.124899   8.860805  -6.699545 -22.198372   0.709553  ...   -27.289408   \n",
            "3   4.306708   2.743538   0.126673 -22.515034   4.918395  ...   -20.318048   \n",
            "4  10.648178 -10.684217 -14.625661 -17.181250   4.953502  ...   -20.582487   \n",
            "\n",
            "   feature_503  feature_504  feature_505  feature_506  feature_507  \\\n",
            "0    14.943659    13.477428    -4.155623    -9.507592     8.618485   \n",
            "1    10.771814    -5.747799    -3.334548    -9.736807     6.738619   \n",
            "2    11.609870     8.180714    -3.151415    -2.993749     8.061224   \n",
            "3    10.032331     6.393432    -3.299681    -1.061766    12.650619   \n",
            "4    10.982810     0.340775    -3.794086     8.385500     6.828188   \n",
            "\n",
            "   feature_508  feature_509  feature_510  feature_511  \n",
            "0    13.069713    10.509280    12.326131   -17.774620  \n",
            "1    17.352596     1.551362    10.680675   -19.057421  \n",
            "2     3.734898    10.704810    12.569386   -15.976426  \n",
            "3    12.811735     5.153952    10.701712   -21.488647  \n",
            "4    10.967863     6.039034    13.918029   -21.615765  \n",
            "\n",
            "[5 rows x 514 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "understanding the blog code\n"
      ],
      "metadata": {
        "id": "q5ghrTXHgfTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(mnist_train, mnist_test), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "def transform_labels(image, label):\n",
        "  return image, tf.math.floor(label / 2)\n",
        "\n",
        "def prepare(ds, shuffle=True, batch_size=32, prefetch=True):\n",
        "  ds = ds.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.map(transform_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.shuffle(ds_info.splits['train'].num_examples) if shuffle else ds\n",
        "  ds = ds.cache()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE) if prefetch else ds\n",
        "  return ds\n",
        "\n",
        "def split_tasks(ds, predicate):\n",
        "  return ds.filter(predicate), ds.filter(lambda img, label: not predicate(img, label))\n",
        "\n",
        "multi_task_train, multi_task_test = prepare(mnist_train), prepare(mnist_test)\n",
        "task_A_train, task_B_train = split_tasks(mnist_train, lambda img, label: label % 2 == 0)\n",
        "task_A_train, task_B_train = prepare(task_A_train), prepare(task_B_train)\n",
        "task_A_test, task_B_test = split_tasks(mnist_test, lambda img, label: label % 2 == 0)\n",
        "task_A_test, task_B_test = prepare(task_A_test), prepare(task_B_test)\n",
        "\n",
        "def evaluate(model, test_set):\n",
        "  acc = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "  for i, (imgs, labels) in enumerate(test_set):\n",
        "    preds = model.predict_on_batch(imgs)\n",
        "    acc.update_state(labels, preds)\n",
        "  return acc.result().numpy()\n",
        "\n",
        "multi_task_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(5)\n",
        "])\n",
        "\n",
        "multi_task_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy')\n",
        "\n",
        "multi_task_model.fit(multi_task_train, epochs=6)\n",
        "\n",
        "print(\"Task A accuracy after training on Multi-Task Problem: {}\".format(evaluate(multi_task_model, task_A_test)))\n",
        "print(\"Task B accuracy after training on Multi-Task Problem: {}\".format(evaluate(multi_task_model, task_B_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "5e799565ab6c4ffa8605de93fe7a5213",
            "ab75738523c24229802a729ffd1d409a",
            "c6e8444a0fbc41fba49e64d640b3bbd1",
            "c54b0ee8da1b48148a668bf8686c7434",
            "feffe3ca3396475598916362eea8439a",
            "f326d871e6c34c90ae200cfa8430ea6d",
            "4c73b8db04a54ff8baa616025bc35e67",
            "83bf83ced001496e98c259954017edc1",
            "0523566155ef47ff9766d587e3ed45b0",
            "670275126ab0496eab80a69ff4396a24",
            "8458db3e39dd4888b90eba7c7029b7dc"
          ]
        },
        "id": "Q8xpMF8AgheV",
        "outputId": "a3db4d35-22c6-4763-c421-4ee993bf5a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e799565ab6c4ffa8605de93fe7a5213"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 16s 5ms/step - loss: 0.2257 - accuracy: 0.9306\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1027 - accuracy: 0.9688\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0728 - accuracy: 0.9774\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0543 - accuracy: 0.9839\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0413 - accuracy: 0.9881\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0313 - accuracy: 0.9916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7908cd76c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A accuracy after training on Multi-Task Problem: 0.9803085923194885\n",
            "Task B accuracy after training on Multi-Task Problem: 0.9716200232505798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_cl_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(5)\n",
        "])\n",
        "\n",
        "basic_cl_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy')\n",
        "\n",
        "basic_cl_model.fit(task_A_train, epochs=6)\n",
        "\n",
        "print(\"Task A accuracy after training model on only Task A: {}\".format(evaluate(basic_cl_model, task_A_test)))\n",
        "\n",
        "basic_cl_model.fit(task_B_train, epochs=6)\n",
        "\n",
        "print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(basic_cl_model, task_B_test)))\n",
        "print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(basic_cl_model, task_A_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQETSglvgi0Z",
        "outputId": "d2e2f98e-ab6e-4c8f-fa22-0ca69b7f8562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "922/922 [==============================] - 6s 3ms/step - loss: 0.1384 - accuracy: 0.9592\n",
            "Epoch 2/6\n",
            "922/922 [==============================] - 3s 4ms/step - loss: 0.0569 - accuracy: 0.9829\n",
            "Epoch 3/6\n",
            "922/922 [==============================] - 3s 3ms/step - loss: 0.0358 - accuracy: 0.9895\n",
            "Epoch 4/6\n",
            "922/922 [==============================] - 4s 5ms/step - loss: 0.0238 - accuracy: 0.9935\n",
            "Epoch 5/6\n",
            "922/922 [==============================] - 3s 3ms/step - loss: 0.0166 - accuracy: 0.9963\n",
            "Epoch 6/6\n",
            "922/922 [==============================] - 3s 3ms/step - loss: 0.0111 - accuracy: 0.9974\n",
            "Task A accuracy after training model on only Task A: 0.986804723739624\n",
            "Epoch 1/6\n",
            "954/954 [==============================] - 7s 4ms/step - loss: 0.1935 - accuracy: 0.9487\n",
            "Epoch 2/6\n",
            "954/954 [==============================] - 4s 4ms/step - loss: 0.0641 - accuracy: 0.9809\n",
            "Epoch 3/6\n",
            "954/954 [==============================] - 4s 4ms/step - loss: 0.0427 - accuracy: 0.9880\n",
            "Epoch 4/6\n",
            "954/954 [==============================] - 3s 3ms/step - loss: 0.0302 - accuracy: 0.9913\n",
            "Epoch 5/6\n",
            "954/954 [==============================] - 3s 3ms/step - loss: 0.0217 - accuracy: 0.9948\n",
            "Epoch 6/6\n",
            "954/954 [==============================] - 4s 5ms/step - loss: 0.0150 - accuracy: 0.9968\n",
            "Task B accuracy after training trained model on Task B: 0.9840362668037415\n",
            "Task A accuracy after training trained model on Task B: 0.26228177547454834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_penalty(theta, theta_A):\n",
        "  penalty = 0\n",
        "  for i, theta_i in enumerate(theta):\n",
        "    _penalty = tf.math.reduce_sum((theta_i - theta_A[i]) ** 2)\n",
        "    penalty += _penalty\n",
        "  return 0.5*penalty\n",
        "\n",
        "def train_with_l2(model, task_A_train, task_B_train, task_A_test, task_B_test, epochs=6):\n",
        "  model.fit(task_A_train, epochs=epochs)\n",
        "  theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
        "\n",
        "  print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, task_A_test)))\n",
        "  # Now we set up the training loop for task B with EWC\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "  loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
        "\n",
        "  for epoch in range(epochs*3):\n",
        "    accuracy.reset_states()\n",
        "    loss.reset_states()\n",
        "\n",
        "    for batch, (imgs, labels) in enumerate(task_B_set):\n",
        "      with tf.GradientTape() as tape:\n",
        "        # Make the predictions\n",
        "        preds = model(imgs)\n",
        "        # Compute EWC loss\n",
        "        total_loss = ewc_loss(labels, preds, model, F, theta_A)\n",
        "      # Compute the gradients of model's trainable parameters wrt total loss\n",
        "      grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "      # Update the model with gradients\n",
        "      model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      # Report updated loss and accuracy\n",
        "      accuracy.update_state(labels, preds)\n",
        "      loss.update_state(labels, preds)\n",
        "      print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
        "          epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
        "         )\n",
        "    print(\"\")\n",
        "\n",
        "  print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, task_B_test)))\n",
        "  print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, task_A_test)))\n",
        "\n",
        "# Define EWC model\n",
        "ewc_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(5)\n",
        "])"
      ],
      "metadata": {
        "id": "7E78eMp_gi2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(5)\n",
        "])\n",
        "\n",
        "l2_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy')\n",
        "\n",
        "train_with_l2(l2_model, task_A_train, task_B_train, task_A_test, task_B_test)\n",
        "\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "  task_set = task_set.repeat()\n",
        "  precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "  for i, (imgs, labels) in enumerate(task_set.take(num_batches)):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(imgs)\n",
        "      ll = tf.nn.log_softmax(preds)\n",
        "    ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "    for i, g in enumerate(ll_grads):\n",
        "      precision_matrices[i] += tf.math.reduce_mean(g ** 2, axis=0) / num_batches\n",
        "\n",
        "  return precision_matrices\n",
        "\n",
        "def compute_elastic_penalty(F, theta, theta_A, alpha=25):\n",
        "  penalty = 0\n",
        "  for i, theta_i in enumerate(theta):\n",
        "    _penalty = tf.math.reduce_sum(F[i] * (theta_i - theta_A[i]) ** 2)\n",
        "    penalty += _penalty\n",
        "  return 0.5*alpha*penalty\n",
        "\n",
        "def ewc_loss(labels, preds, model, F, theta_A):\n",
        "  loss_b = model.loss(labels, preds)\n",
        "  penalty = compute_elastic_penalty(F, model.trainable_variables, theta_A)\n",
        "  return loss_b + penalty\n",
        "\n",
        "def train_with_ewc(model, task_A_set, task_B_set, task_A_test, task_B_test, epochs=3):\n",
        "  # First we're going to fit to task A and retain a copy of parameters trained on Task A\n",
        "  model.fit(task_A_set, epochs=epochs)\n",
        "  theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
        "  # We'll only compute Fisher once, you can do it whenever\n",
        "  F = compute_precision_matrices(model, task_A_set, num_batches=1000)\n",
        "\n",
        "  print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, task_A_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "ZyXAczGxgwPp",
        "outputId": "56087c2e-01d1-4df0-f00f-c32761253539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "922/922 [==============================] - 5s 5ms/step - loss: 0.1333 - accuracy: 0.9614\n",
            "Epoch 2/6\n",
            "922/922 [==============================] - 3s 3ms/step - loss: 0.0556 - accuracy: 0.9836\n",
            "Epoch 3/6\n",
            "922/922 [==============================] - 3s 3ms/step - loss: 0.0357 - accuracy: 0.9891\n",
            "Epoch 4/6\n",
            "922/922 [==============================] - 4s 4ms/step - loss: 0.0240 - accuracy: 0.9936\n",
            "Epoch 5/6\n",
            "922/922 [==============================] - 3s 3ms/step - loss: 0.0159 - accuracy: 0.9963\n",
            "Epoch 6/6\n",
            "922/922 [==============================] - 4s 5ms/step - loss: 0.0110 - accuracy: 0.9978\n",
            "Task A accuracy after training on Task A: 0.9839626550674438\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'task_B_set' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7eee6efb386a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ml2_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_with_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_A_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_B_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_A_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_B_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_precision_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-a9a50d8bcdb2>\u001b[0m in \u001b[0;36mtrain_with_l2\u001b[0;34m(model, task_A_train, task_B_train, task_A_test, task_B_test, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_B_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Make the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'task_B_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we set up the training loop for task B with EWC\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "  loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
        "\n",
        "  for epoch in range(epochs*3):\n",
        "    accuracy.reset_states()\n",
        "    loss.reset_states()\n",
        "\n",
        "    for batch, (imgs, labels) in enumerate(task_B_set):\n",
        "      with tf.GradientTape() as tape:\n",
        "        # Make the predictions\n",
        "        preds = model(imgs)\n",
        "        # Compute EWC loss\n",
        "        total_loss = ewc_loss(labels, preds, model, F, theta_A)\n",
        "      # Compute the gradients of model's trainable parameters wrt total loss\n",
        "      grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "      # Update the model with gradients\n",
        "      model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      # Report updated loss and accuracy\n",
        "      accuracy.update_state(labels, preds)\n",
        "      loss.update_state(labels, preds)\n",
        "      print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
        "          epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
        "         )\n",
        "    print(\"\")\n",
        "\n",
        "  print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, task_B_test)))\n",
        "  print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, task_A_test)))\n",
        "\n",
        "# Define EWC model\n",
        "ewc_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(5)\n",
        "])"
      ],
      "metadata": {
        "id": "iXpkArzdgi7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQQS71vQgi9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1st step of code implementation**"
      ],
      "metadata": {
        "id": "5Kcs4bcPeYh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "vl0uYTeqovgy",
        "outputId": "685ce3e2-f7f8-44df-84e6-2668e022c3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        name  Class  feature_0  feature_1  feature_2  \\\n",
              "0    Reading_depressed_0.wav      1 -23.453386  -6.972407  10.922126   \n",
              "1    Reading_depressed_1.wav      1 -25.416334  -9.199664  10.101542   \n",
              "2   Reading_depressed_10.wav      1 -28.969011  -5.609468  12.488464   \n",
              "3  Reading_depressed_100.wav      1 -19.701765  12.261052   7.070023   \n",
              "4  Reading_depressed_101.wav      1 -22.105116  -7.490783  10.399014   \n",
              "\n",
              "   feature_3  feature_4  feature_5  feature_6  feature_7  ...  feature_502  \\\n",
              "0   8.391150  -0.868982 -18.249210 -12.701187  -4.695340  ...   -20.443972   \n",
              "1  10.495510   3.363707  -7.523287 -24.693653  -5.431184  ...   -25.981260   \n",
              "2  16.134411  15.011572 -12.213967 -19.554272  -4.658757  ...   -24.819601   \n",
              "3  13.615430  11.327873   1.511992 -23.616589   9.273006  ...   -22.441940   \n",
              "4  10.492347  -1.390268 -12.003224 -16.849686  -1.143140  ...   -19.453552   \n",
              "\n",
              "   feature_503  feature_504  feature_505  feature_506  feature_507  \\\n",
              "0     7.936236    13.323593    -4.330662    -7.336786    14.697271   \n",
              "1    10.707705    17.087761    -3.575731    -4.642745    10.044394   \n",
              "2    13.386227    -3.435347    -4.213245   -17.756733    12.055525   \n",
              "3     9.820329   -11.617693    -4.013164    -9.241024     8.637423   \n",
              "4    13.122263    17.303165    -3.531888   -10.645234    12.287755   \n",
              "\n",
              "   feature_508  feature_509  feature_510  feature_511  \n",
              "0    14.230615    15.683101    16.694450   -17.903179  \n",
              "1     7.741048    16.284454    16.271326   -14.527636  \n",
              "2    12.868338     7.721011    14.429776   -17.652357  \n",
              "3     8.017566    -0.484947    13.701614    -9.746015  \n",
              "4     5.714491    13.738103    10.087270   -14.942650  \n",
              "\n",
              "[5 rows x 514 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e227248a-bf57-417a-991c-2a1157d945a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>Class</th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_502</th>\n",
              "      <th>feature_503</th>\n",
              "      <th>feature_504</th>\n",
              "      <th>feature_505</th>\n",
              "      <th>feature_506</th>\n",
              "      <th>feature_507</th>\n",
              "      <th>feature_508</th>\n",
              "      <th>feature_509</th>\n",
              "      <th>feature_510</th>\n",
              "      <th>feature_511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reading_depressed_0.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>-23.453386</td>\n",
              "      <td>-6.972407</td>\n",
              "      <td>10.922126</td>\n",
              "      <td>8.391150</td>\n",
              "      <td>-0.868982</td>\n",
              "      <td>-18.249210</td>\n",
              "      <td>-12.701187</td>\n",
              "      <td>-4.695340</td>\n",
              "      <td>...</td>\n",
              "      <td>-20.443972</td>\n",
              "      <td>7.936236</td>\n",
              "      <td>13.323593</td>\n",
              "      <td>-4.330662</td>\n",
              "      <td>-7.336786</td>\n",
              "      <td>14.697271</td>\n",
              "      <td>14.230615</td>\n",
              "      <td>15.683101</td>\n",
              "      <td>16.694450</td>\n",
              "      <td>-17.903179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Reading_depressed_1.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>-25.416334</td>\n",
              "      <td>-9.199664</td>\n",
              "      <td>10.101542</td>\n",
              "      <td>10.495510</td>\n",
              "      <td>3.363707</td>\n",
              "      <td>-7.523287</td>\n",
              "      <td>-24.693653</td>\n",
              "      <td>-5.431184</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.981260</td>\n",
              "      <td>10.707705</td>\n",
              "      <td>17.087761</td>\n",
              "      <td>-3.575731</td>\n",
              "      <td>-4.642745</td>\n",
              "      <td>10.044394</td>\n",
              "      <td>7.741048</td>\n",
              "      <td>16.284454</td>\n",
              "      <td>16.271326</td>\n",
              "      <td>-14.527636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reading_depressed_10.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>-28.969011</td>\n",
              "      <td>-5.609468</td>\n",
              "      <td>12.488464</td>\n",
              "      <td>16.134411</td>\n",
              "      <td>15.011572</td>\n",
              "      <td>-12.213967</td>\n",
              "      <td>-19.554272</td>\n",
              "      <td>-4.658757</td>\n",
              "      <td>...</td>\n",
              "      <td>-24.819601</td>\n",
              "      <td>13.386227</td>\n",
              "      <td>-3.435347</td>\n",
              "      <td>-4.213245</td>\n",
              "      <td>-17.756733</td>\n",
              "      <td>12.055525</td>\n",
              "      <td>12.868338</td>\n",
              "      <td>7.721011</td>\n",
              "      <td>14.429776</td>\n",
              "      <td>-17.652357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Reading_depressed_100.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>-19.701765</td>\n",
              "      <td>12.261052</td>\n",
              "      <td>7.070023</td>\n",
              "      <td>13.615430</td>\n",
              "      <td>11.327873</td>\n",
              "      <td>1.511992</td>\n",
              "      <td>-23.616589</td>\n",
              "      <td>9.273006</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.441940</td>\n",
              "      <td>9.820329</td>\n",
              "      <td>-11.617693</td>\n",
              "      <td>-4.013164</td>\n",
              "      <td>-9.241024</td>\n",
              "      <td>8.637423</td>\n",
              "      <td>8.017566</td>\n",
              "      <td>-0.484947</td>\n",
              "      <td>13.701614</td>\n",
              "      <td>-9.746015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reading_depressed_101.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>-22.105116</td>\n",
              "      <td>-7.490783</td>\n",
              "      <td>10.399014</td>\n",
              "      <td>10.492347</td>\n",
              "      <td>-1.390268</td>\n",
              "      <td>-12.003224</td>\n",
              "      <td>-16.849686</td>\n",
              "      <td>-1.143140</td>\n",
              "      <td>...</td>\n",
              "      <td>-19.453552</td>\n",
              "      <td>13.122263</td>\n",
              "      <td>17.303165</td>\n",
              "      <td>-3.531888</td>\n",
              "      <td>-10.645234</td>\n",
              "      <td>12.287755</td>\n",
              "      <td>5.714491</td>\n",
              "      <td>13.738103</td>\n",
              "      <td>10.087270</td>\n",
              "      <td>-14.942650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 514 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e227248a-bf57-417a-991c-2a1157d945a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e227248a-bf57-417a-991c-2a1157d945a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e227248a-bf57-417a-991c-2a1157d945a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-028d0128-9f9c-4a31-8b7b-09c9602546ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-028d0128-9f9c-4a31-8b7b-09c9602546ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-028d0128-9f9c-4a31-8b7b-09c9602546ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and process the first audio dataset\n",
        "audio_train_path_1 = \"/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv\"\n",
        "audio_test_path_1 = \"/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv\"\n",
        "\n",
        "audio_train_df_1 = pd.read_csv(audio_train_path_1)\n",
        "audio_test_df_1 = pd.read_csv(audio_test_path_1)\n",
        "\n",
        "# Load and process the second audio dataset\n",
        "audio_train_path_2 = \"/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv\"\n",
        "audio_test_path_2 = \"/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv\"\n",
        "\n",
        "audio_train_df_2 = pd.read_csv(audio_train_path_2)\n",
        "audio_test_df_2 = pd.read_csv(audio_test_path_2)\n",
        "\n",
        "# Randomly sample rows for training and testing from the first dataset\n",
        "audio_train_df_1 = audio_train_df_1.sample(frac=1).reset_index(drop=True)\n",
        "audio_test_df_1 = audio_test_df_1.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Randomly sample rows for training and testing from the second dataset\n",
        "audio_train_df_2 = audio_train_df_2.sample(frac=1).reset_index(drop=True)\n",
        "audio_test_df_2 = audio_test_df_2.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Convert audio features to numpy arrays for the first dataset\n",
        "X_train_1 = audio_train_df_1.drop(columns=['name']).values.astype(np.float32)\n",
        "y_train_1 = audio_train_df_1['Class'].values.astype(np.int32)  # Convert labels to integers\n",
        "X_test_1 = audio_test_df_1.drop(columns=['name']).values.astype(np.float32)\n",
        "y_test_1 = audio_test_df_1['Class'].values.astype(np.int32)  # Convert labels to integers\n",
        "\n",
        "# Convert audio features to numpy arrays for the second dataset\n",
        "X_train_2 = audio_train_df_2.drop(columns=['name']).values.astype(np.float32)\n",
        "y_train_2 = audio_train_df_2['Class'].values.astype(np.int32)  # Convert labels to integers\n",
        "X_test_2 = audio_test_df_2.drop(columns=['name']).values.astype(np.float32)\n",
        "y_test_2 = audio_test_df_2['Class'].values.astype(np.int32)  # Convert labels to integers\n",
        "\n",
        "# Convert audio datasets to TensorFlow datasets for the first dataset\n",
        "audio_train_ds_1 = tf.data.Dataset.from_tensor_slices((X_train_1, y_train_1))\n",
        "audio_test_ds_1 = tf.data.Dataset.from_tensor_slices((X_test_1, y_test_1))\n",
        "\n",
        "# Convert audio datasets to TensorFlow datasets for the second dataset\n",
        "audio_train_ds_2 = tf.data.Dataset.from_tensor_slices((X_train_2, y_train_2))\n",
        "audio_test_ds_2 = tf.data.Dataset.from_tensor_slices((X_test_2, y_test_2))\n",
        "\n",
        "# Shuffle and batch the datasets for the first dataset\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_SIZE = 1000\n",
        "audio_train_ds_1 = audio_train_ds_1.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)\n",
        "audio_test_ds_1 = audio_test_ds_1.batch(BATCH_SIZE)\n",
        "\n",
        "# Shuffle and batch the datasets for the second dataset\n",
        "audio_train_ds_2 = audio_train_ds_2.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)\n",
        "audio_test_ds_2 = audio_test_ds_2.batch(BATCH_SIZE)\n",
        "\n",
        "# Load your pre-trained CNN model\n",
        "model_path = '/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5'\n",
        "cnn_model = tf.keras.models.load_model(model_path)\n",
        "print(cnn_model.summary())\n",
        "\n",
        "# Fine-tune your CNN model on the first dataset\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(audio_train_ds_1, epochs=10)\n",
        "\n",
        "# Evaluate your CNN model on the first dataset\n",
        "test_loss_1, test_accuracy_1 = cnn_model.evaluate(audio_test_ds_1)\n",
        "print(\"Test Loss (Dataset 1):\", test_loss_1)\n",
        "print(\"Test Accuracy (Dataset 1):\", test_accuracy_1)\n",
        "\n",
        "# Fine-tune your CNN model on the second dataset\n",
        "cnn_model.fit(audio_train_ds_2, epochs=10)\n",
        "\n",
        "# Evaluate your CNN model on the second dataset\n",
        "test_loss_2, test_accuracy_2 = cnn_model.evaluate(audio_test_ds_2)\n",
        "print(\"Test Loss (Dataset 2):\", test_loss_2)\n",
        "print(\"Test Accuracy (Dataset 2):\", test_accuracy_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FnqL5G98en0b",
        "outputId": "9ec17501-0b4e-4e79-b603-9e1ea5699407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-dc06bdd41489>:29: RuntimeWarning: invalid value encountered in cast\n",
            "  y_train_1 = audio_train_df_1['Class'].values.astype(np.int32)  # Convert labels to integers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 510, 32)           128       \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 255, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 253, 64)           6208      \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 126, 64)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 8064)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               1032320   \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1038785 (3.96 MB)\n",
            "Trainable params: 1038785 (3.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 512, 1), found shape=(None, 513)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dc06bdd41489>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Fine-tune your CNN model on the first dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_train_ds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Evaluate your CNN model on the first dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 512, 1), found shape=(None, 513)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and process the first audio dataset\n",
        "audio_train_path_1 = \"/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv\"\n",
        "audio_test_path_1 = \"/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv\"\n",
        "\n",
        "audio_train_df_1 = pd.read_csv(audio_train_path_1)\n",
        "audio_test_df_1 = pd.read_csv(audio_test_path_1)\n",
        "\n",
        "# Load and process the second audio dataset\n",
        "audio_train_path_2 = \"/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv\"\n",
        "audio_test_path_2 = \"/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv\"\n",
        "\n",
        "audio_train_df_2 = pd.read_csv(audio_train_path_2)\n",
        "audio_test_df_2 = pd.read_csv(audio_test_path_2)\n",
        "\n",
        "# Convert audio features to numpy arrays for the first dataset\n",
        "X_train_1 = audio_train_df_1.drop(columns=['name']).values.astype(np.float32)\n",
        "y_train_1 = audio_train_df_1['Class'].values.astype(np.float32)\n",
        "X_test_1 = audio_test_df_1.drop(columns=['name']).values.astype(np.float32)\n",
        "y_test_1 = audio_test_df_1['Class'].values.astype(np.float32)\n",
        "\n",
        "# Convert audio features to numpy arrays for the second dataset\n",
        "X_train_2 = audio_train_df_2.drop(columns=['name']).values.astype(np.float32)\n",
        "y_train_2 = audio_train_df_2['Class'].values.astype(np.float32)\n",
        "X_test_2 = audio_test_df_2.drop(columns=['name']).values.astype(np.float32)\n",
        "y_test_2 = audio_test_df_2['Class'].values.astype(np.float32)\n",
        "\n",
        "# Reshape audio features for CNN input (assuming 1D convolution)\n",
        "X_train_1 = X_train_1.reshape(-1, X_train_1.shape[1], 1)\n",
        "X_test_1 = X_test_1.reshape(-1, X_test_1.shape[1], 1)\n",
        "X_train_2 = X_train_2.reshape(-1, X_train_2.shape[1], 1)\n",
        "X_test_2 = X_test_2.reshape(-1, X_test_2.shape[1], 1)\n",
        "\n",
        "# Convert audio datasets to TensorFlow datasets for the first dataset\n",
        "audio_train_ds_1 = tf.data.Dataset.from_tensor_slices((X_train_1, y_train_1))\n",
        "audio_test_ds_1 = tf.data.Dataset.from_tensor_slices((X_test_1, y_test_1))\n",
        "\n",
        "# Convert audio datasets to TensorFlow datasets for the second dataset\n",
        "audio_train_ds_2 = tf.data.Dataset.from_tensor_slices((X_train_2, y_train_2))\n",
        "audio_test_ds_2 = tf.data.Dataset.from_tensor_slices((X_test_2, y_test_2))\n",
        "\n",
        "# Shuffle and batch the datasets for the first dataset\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_SIZE = 1000\n",
        "audio_train_ds_1 = audio_train_ds_1.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)\n",
        "audio_test_ds_1 = audio_test_ds_1.batch(BATCH_SIZE)\n",
        "\n",
        "# Shuffle and batch the datasets for the second dataset\n",
        "audio_train_ds_2 = audio_train_ds_2.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)\n",
        "audio_test_ds_2 = audio_test_ds_2.batch(BATCH_SIZE)\n",
        "\n",
        "# Load your model\n",
        "model_path = '/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5'\n",
        "cnn_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Train your CNN model on the first dataset\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(audio_train_ds_1, epochs=10)\n",
        "\n",
        "# Evaluate your model on the first dataset\n",
        "test_loss_1, test_accuracy_1 = cnn_model.evaluate(audio_test_ds_1)\n",
        "print(\"Test Loss (Dataset 1):\", test_loss_1)\n",
        "print(\"Test Accuracy (Dataset 1):\", test_accuracy_1)\n",
        "\n",
        "# Train your CNN model on the second dataset\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(audio_train_ds_2, epochs=10)\n",
        "\n",
        "# Evaluate your model on the second dataset\n",
        "test_loss_2, test_accuracy_2 = cnn_model.evaluate(audio_test_ds_2)\n",
        "print(\"Test Loss (Dataset 2):\", test_loss_2)\n",
        "print(\"Test Accuracy (Dataset 2):\", test_accuracy_2)\n"
      ],
      "metadata": {
        "id": "FTGZb3-FSfXL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "db0faadb-5396-481d-c63e-c61a0d06d9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 512, 1), found shape=(None, 513, 1)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-132bc0b1e0b3>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Train your CNN model on the first dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_train_ds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Evaluate your model on the first dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 512, 1), found shape=(None, 513, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Codes**"
      ],
      "metadata": {
        "id": "4_DnpBAwSfih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Catestrophic Forgetting**"
      ],
      "metadata": {
        "id": "L3fCYcwtSiM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading And Testing CNN model on EDAIC"
      ],
      "metadata": {
        "id": "tOJOmIXJSzGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkk1AprBSmfM",
        "outputId": "08637361-3cfb-433c-c540-ceff2e1bb3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 1s 9ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.7522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on Interview Dataset"
      ],
      "metadata": {
        "id": "1yPdhJ0RTCST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk4TeZeOTCc1",
        "outputId": "c33d157c-3723-457a-d65b-4bdf0d99c1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "47/47 [==============================] - 1s 23ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.4323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Fine-tune on Interview"
      ],
      "metadata": {
        "id": "AF5EjxMJTMvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the training and validation dataset\n",
        "interview_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv')\n",
        "\n",
        "# Prepare the dataset by dropping the 'name' column and separating features and labels\n",
        "X = interview_df.drop(columns=['name', 'Class']).values\n",
        "y = interview_df['Class'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/Depression/Model/latest_CNN.h5')\n",
        "\n",
        "# Compile the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fine-tune the model on the new dataset\n",
        "model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=10, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnjnApYHTM6o",
        "outputId": "43d362fa-d219-4474-fe63-29441f23e4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "85/85 [==============================] - 6s 53ms/step - loss: 1.7632 - accuracy: 0.5564 - val_loss: 0.8433 - val_accuracy: 0.5742\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 3s 37ms/step - loss: 0.7691 - accuracy: 0.5405 - val_loss: 0.6793 - val_accuracy: 0.6024\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 3s 35ms/step - loss: 0.6827 - accuracy: 0.5750 - val_loss: 0.6384 - val_accuracy: 0.6320\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.6486 - accuracy: 0.6162 - val_loss: 0.5954 - val_accuracy: 0.7136\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 6s 65ms/step - loss: 0.6033 - accuracy: 0.6841 - val_loss: 0.5627 - val_accuracy: 0.7537\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 6s 68ms/step - loss: 0.5698 - accuracy: 0.7131 - val_loss: 0.5310 - val_accuracy: 0.7774\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 6s 75ms/step - loss: 0.5425 - accuracy: 0.7283 - val_loss: 0.5048 - val_accuracy: 0.7893\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 4s 46ms/step - loss: 0.5145 - accuracy: 0.7450 - val_loss: 0.4746 - val_accuracy: 0.8101\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 5s 64ms/step - loss: 0.4938 - accuracy: 0.7569 - val_loss: 0.4485 - val_accuracy: 0.8264\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 6s 67ms/step - loss: 0.4669 - accuracy: 0.7762 - val_loss: 0.4237 - val_accuracy: 0.8368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Fine-Tuned Model On Interview"
      ],
      "metadata": {
        "id": "yQ8l1uV7TZXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the test dataset\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "\n",
        "# Prepare the test dataset by dropping the 'name' column and separating features and labels\n",
        "X_test = test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = test_df['Class'].values\n",
        "\n",
        "# Scale the features of the test set using the same scaler as used for training\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = load_model('/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5')\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = fine_tuned_model.predict(X_test_scaled)\n",
        "# Convert probabilities to class labels based on a threshold (0.5 for binary classification)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, predicted_classes)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wehX1vyuTZmq",
        "outputId": "bf1bb5d1-2bc3-4e48-9bd4-8ca886b6ae66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 9ms/step\n",
            "Test Accuracy: 0.6504336224149433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Fine-Tuned Model On EDAIC"
      ],
      "metadata": {
        "id": "r6ksPxGqTpYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the test dataset\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "# Prepare the test dataset by dropping the 'name' column and separating features and labels\n",
        "X_test = test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = test_df['Class'].values\n",
        "\n",
        "# Scale the features of the test set using the same scaler as used for training\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = load_model('/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5')\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = fine_tuned_model.predict(X_test_scaled)\n",
        "# Convert probabilities to class labels based on a threshold (0.5 for binary classification)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, predicted_classes)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtOyYJpOTphX",
        "outputId": "295eec07-c4f2-4474-e82e-e39fb18a533b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 1s 9ms/step\n",
            "Test Accuracy: 0.669255928045789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeQwpqAf0uVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5kQdZ1E0uXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkJ63PDO0uZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OQWpufe0ucF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIDCIMOe0ud8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHjCNJh60uf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJbx7g1i0uiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQNY4XHU0ukc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hK1bZ2910umg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lb8-uzkf0uo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XodugEhr0uq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FeySW7Hs0utS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2FlNUZr0uvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbiDtU8j0uxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KV7kX6yY0u0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnW8Kw1Z0u2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **next**"
      ],
      "metadata": {
        "id": "KRvkyiQOVQM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and process the first audio dataset\n",
        "audio_train_path_1 = \"/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv\"  # Update with the path to your first audio train CSV file\n",
        "audio_test_path_1 = \"/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv\"  # Update with the path to your first audio test CSV file\n",
        "\n",
        "audio_train_df_1 = pd.read_csv(audio_train_path_1)\n",
        "audio_test_df_1 = pd.read_csv(audio_test_path_1)\n",
        "\n",
        "# Load and process the second audio dataset\n",
        "audio_train_path_2 = \"/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv\"  # Update with the path to your second audio train CSV file\n",
        "audio_test_path_2 = \"/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv\"  # Update with the path to your second audio test CSV file\n",
        "\n",
        "audio_train_df_2 = pd.read_csv(audio_train_path_2)\n",
        "audio_test_df_2 = pd.read_csv(audio_test_path_2)\n",
        "\n",
        "# Define normalization function for audio features\n",
        "def normalize_audio(audio_features, label):\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_audio_features = scaler.fit_transform(audio_features)\n",
        "    return normalized_audio_features, label\n",
        "\n",
        "# Define function to split tasks for audio datasets\n",
        "def split_audio_tasks(audio_ds, predicate):\n",
        "    return audio_ds.filter(predicate), audio_ds.filter(lambda audio, label: not predicate(audio, label))\n",
        "\n",
        "# Prepare audio datasets\n",
        "def prepare_audio(ds, shuffle=True, batch_size=32, prefetch=True):\n",
        "    ds = ds.map(normalize_audio, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(SHUFFLE_SIZE)\n",
        "    ds = ds.cache()\n",
        "    ds = ds.batch(batch_size)\n",
        "    if prefetch:\n",
        "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Define shuffle size\n",
        "SHUFFLE_SIZE = 1000  # Update with appropriate value\n",
        "\n",
        "# Convert audio datasets to TensorFlow datasets\n",
        "audio_train_ds_1 = tf.data.Dataset.from_tensor_slices((audio_train_df_1.drop(columns=['name']).values, audio_train_df_1['Class'].values))\n",
        "audio_test_ds_1 = tf.data.Dataset.from_tensor_slices((audio_test_df_1.drop(columns=['name']).values, audio_test_df_1['Class'].values))\n",
        "\n",
        "audio_train_ds_2 = tf.data.Dataset.from_tensor_slices((audio_train_df_2.drop(columns=['name']).values, audio_train_df_2['Class'].values))\n",
        "audio_test_ds_2 = tf.data.Dataset.from_tensor_slices((audio_test_df_2.drop(columns=['name']).values, audio_test_df_2['Class'].values))\n",
        "\n",
        "# Split tasks for audio datasets if needed\n",
        "# task_A_train, task_B_train = split_audio_tasks(audio_train_ds, YOUR_PREDICATE_FUNCTION)\n",
        "# task_A_test, task_B_test = split_audio_tasks(audio_test_ds, YOUR_PREDICATE_FUNCTION)\n",
        "\n",
        "# Prepare audio datasets\n",
        "# task_A_train = prepare_audio(task_A_train)\n",
        "# task_B_train = prepare_audio(task_B_train)\n",
        "# task_A_test = prepare_audio(task_A_test)\n",
        "# task_B_test = prepare_audio(task_B_test)\n",
        "\n",
        "# Load your CNN model\n",
        "cnn_model = tf.keras.models.load_model('/content/drive/MyDrive/Depression/Model/finetune_on_android.h5')\n",
        "\n",
        "# Define evaluation function for audio data\n",
        "def evaluate_audio(model, test_set):\n",
        "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "    for audio_features, labels in test_set:\n",
        "        preds = model.predict(audio_features)\n",
        "        accuracy.update_state(labels, preds)\n",
        "    return accuracy.result().numpy()\n",
        "\n",
        "# Define training function for audio data\n",
        "def train_audio_model(model, train_set, epochs):\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_set, epochs=10)\n",
        "\n",
        "# Define number of epochs\n",
        "EPOCHS = 10  # Update with appropriate value\n",
        "\n",
        "# Train your CNN model on task A\n",
        "train_audio_model(cnn_model, audio_train_ds_1, EPOCHS)\n",
        "\n",
        "# Evaluate on task A\n",
        "task_A_accuracy_1 = evaluate_audio(cnn_model, audio_test_ds_1)\n",
        "print(\"Task A accuracy for dataset 1: {}\".format(task_A_accuracy_1))\n",
        "\n",
        "# Train your CNN model on task B\n",
        "train_audio_model(cnn_model, audio_train_ds_2, EPOCHS)\n",
        "\n",
        "# Evaluate on task B\n",
        "task_B_accuracy_2 = evaluate_audio(cnn_model, audio_test_ds_2)\n",
        "print(\"Task B accuracy for dataset 2: {}\".format(task_B_accuracy_2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "OzYZM_XFZIhS",
        "outputId": "b1ea137a-51e7-4e46-d905-9bd71fc74ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_3' (type Functional).\n    \n    Input 0 of layer \"simple_rnn_1\" is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: (513,)\n    \n    Call arguments received by layer 'model_3' (type Functional):\n      • inputs=tf.Tensor(shape=(513,), dtype=float64)\n      • training=True\n      • mask=None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-50db0b589fb8>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Train your CNN model on task A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_audio_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_train_ds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Evaluate on task A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-50db0b589fb8>\u001b[0m in \u001b[0;36mtrain_audio_model\u001b[0;34m(model, train_set, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_audio_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Define number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_3' (type Functional).\n    \n    Input 0 of layer \"simple_rnn_1\" is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: (513,)\n    \n    Call arguments received by layer 'model_3' (type Functional):\n      • inputs=tf.Tensor(shape=(513,), dtype=float64)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and process the audio dataset\n",
        "audio_train_path = \"/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv\"  # Update with the path to your audio train CSV file\n",
        "audio_test_path = \"/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv\"  # Update with the path to your audio test CSV file\n",
        "\n",
        "audio_train_df = pd.read_csv(audio_train_path)\n",
        "audio_test_df = pd.read_csv(audio_test_path)\n",
        "\n",
        "# Define normalization function for audio features\n",
        "def normalize_audio(audio_features, label):\n",
        "    scaler = MinMaxScaler()\n",
        "    # Convert TensorFlow tensor to NumPy array\n",
        "    audio_features_np = audio_features.numpy()\n",
        "    normalized_audio_features = scaler.fit_transform(audio_features_np)\n",
        "    return normalized_audio_features, label\n",
        "\n",
        "\n",
        "# Convert audio datasets to TensorFlow datasets\n",
        "audio_train_ds = tf.data.Dataset.from_tensor_slices((audio_train_df.drop(columns=['name']).values, audio_train_df['Class'].values))\n",
        "audio_test_ds = tf.data.Dataset.from_tensor_slices((audio_test_df.drop(columns=['name']).values, audio_test_df['Class'].values))\n",
        "\n",
        "# Prepare audio datasets\n",
        "def prepare_audio(ds, shuffle=True, batch_size=32, prefetch=True):\n",
        "    ds = ds.map(normalize_audio, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(SHUFFLE_SIZE)\n",
        "    ds = ds.cache()\n",
        "    ds = ds.batch(batch_size)\n",
        "    if prefetch:\n",
        "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Define shuffle size\n",
        "SHUFFLE_SIZE = 1000  # Update with appropriate value\n",
        "\n",
        "# Prepare audio datasets\n",
        "audio_train_ds = prepare_audio(audio_train_ds)\n",
        "audio_test_ds = prepare_audio(audio_test_ds)\n",
        "\n",
        "# Load your model\n",
        "model_path = '/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5'\n",
        "cnn_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Train your CNN model\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(audio_train_ds, epochs=10)\n",
        "\n",
        "# Evaluate your model\n",
        "test_loss, test_accuracy = cnn_model.evaluate(audio_test_ds)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "wazZ90krbNqp",
        "outputId": "0b2fc7f6-fcaa-4aba-cb14-5ef454237a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "in user code:\n\n    File \"<ipython-input-3-7c685e61f090>\", line 16, in normalize_audio  *\n        audio_features_np = audio_features.numpy()\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'numpy'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7c685e61f090>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Prepare audio datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0maudio_train_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_train_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0maudio_test_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_test_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7c685e61f090>\u001b[0m in \u001b[0;36mprepare_audio\u001b[0;34m(ds, shuffle, batch_size, prefetch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Prepare audio datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHUFFLE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filenls2z0o9.py\u001b[0m in \u001b[0;36mtf__normalize_audio\u001b[0;34m(audio_features, label)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0maudio_features_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mnormalized_audio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_enable_numpy_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \"\"\")\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"<ipython-input-3-7c685e61f090>\", line 16, in normalize_audio  *\n        audio_features_np = audio_features.numpy()\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'numpy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model details"
      ],
      "metadata": {
        "id": "WMxuuvhdatsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the model\n",
        "model_path = '/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5'\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Print the model summary\n",
        "print(loaded_model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DnVqZnzbGWI",
        "outputId": "8bfb99f8-d390-473c-93f6-7a84fedd63f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 510, 32)           128       \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 255, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 253, 64)           6208      \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 126, 64)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 8064)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               1032320   \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1038785 (3.96 MB)\n",
            "Trainable params: 1038785 (3.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_model_details(model_path):\n",
        "    # Load the model\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # Print model architecture\n",
        "    print(\"Model Architecture:\")\n",
        "    print(model.summary())\n",
        "\n",
        "    # Print input shape\n",
        "    print(\"\\nInput Shape:\")\n",
        "    print(model.input_shape)\n",
        "\n",
        "    # Print details of each layer\n",
        "    print(\"\\nLayer Details:\")\n",
        "    for layer in model.layers:\n",
        "        print(layer.name, \"-\", layer.__class__.__name__)\n",
        "\n",
        "        # If it's a convolutional layer, print kernel size and number of filters\n",
        "        if isinstance(layer, tf.keras.layers.Conv1D):\n",
        "            print(\"  Kernel Size:\", layer.kernel_size)\n",
        "            print(\"  Filters:\", layer.filters)\n",
        "\n",
        "        # If it's a recurrent layer, print units\n",
        "        elif isinstance(layer, tf.keras.layers.RNN) or isinstance(layer, tf.keras.layers.LSTM) or isinstance(layer, tf.keras.layers.GRU):\n",
        "            print(\"  Units:\", layer.units)\n",
        "\n",
        "        # If it's a dense layer, print units\n",
        "        elif isinstance(layer, tf.keras.layers.Dense):\n",
        "            print(\"  Units:\", layer.units)\n",
        "\n",
        "        # If it's a dropout layer, print dropout rate\n",
        "        elif isinstance(layer, tf.keras.layers.Dropout):\n",
        "            print(\"  Rate:\", layer.rate)\n",
        "\n",
        "        # If it's any other layer, print its class name\n",
        "        else:\n",
        "            print(\"  Class:\", layer.__class__.__name__)\n",
        "\n",
        "        print()\n",
        "\n",
        "# Input your model path here\n",
        "model_path = '/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5'\n",
        "\n",
        "# Get model details\n",
        "get_model_details(model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZqK508hZIlP",
        "outputId": "06df30c4-82ce-4509-84f5-79ffc7a7e4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 510, 32)           128       \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 255, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 253, 64)           6208      \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 126, 64)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 8064)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               1032320   \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1038785 (3.96 MB)\n",
            "Trainable params: 1038785 (3.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Input Shape:\n",
            "(None, 512, 1)\n",
            "\n",
            "Layer Details:\n",
            "input_7 - InputLayer\n",
            "  Class: InputLayer\n",
            "\n",
            "conv1d_9 - Conv1D\n",
            "  Kernel Size: (3,)\n",
            "  Filters: 32\n",
            "\n",
            "max_pooling1d_9 - MaxPooling1D\n",
            "  Class: MaxPooling1D\n",
            "\n",
            "conv1d_10 - Conv1D\n",
            "  Kernel Size: (3,)\n",
            "  Filters: 64\n",
            "\n",
            "max_pooling1d_10 - MaxPooling1D\n",
            "  Class: MaxPooling1D\n",
            "\n",
            "flatten_6 - Flatten\n",
            "  Class: Flatten\n",
            "\n",
            "dense_12 - Dense\n",
            "  Units: 128\n",
            "\n",
            "dropout_15 - Dropout\n",
            "  Rate: 0.5\n",
            "\n",
            "dense_13 - Dense\n",
            "  Units: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **L1 AND L2**"
      ],
      "metadata": {
        "id": "x7Pd1uXY8-sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Conv1D, Dense\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# Define the path where your model is saved\n",
        "model_path = '/content/drive/MyDrive/Depression/Model/finetune_on_latest.h5'\n",
        "\n",
        "# Load the model\n",
        "fine_tuned_model = load_model(model_path)\n",
        "\n",
        "# Define regularization strength\n",
        "l1_value = 0.1\n",
        "l2_value = 0.1\n",
        "\n",
        "# Apply L1 and L2 regularization to each layer\n",
        "for layer in fine_tuned_model.layers:\n",
        "    if isinstance(layer, Conv1D) or isinstance(layer, Dense):\n",
        "        layer.kernel_regularizer = l1_l2(l1=l1_value, l2=l2_value)\n",
        "\n",
        "# Compile the model\n",
        "fine_tuned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the path where your updated model will be saved\n",
        "save_path = \"/content/drive/MyDrive/Depression/Model/l1.h5\"\n",
        "\n",
        "# Save the updated model\n",
        "fine_tuned_model.save(save_path)\n",
        "\n",
        "print(\"Updated model with L1 and L2 regularization applied has been saved as 'l1.h5'\")\n"
      ],
      "metadata": {
        "id": "ZgCP9hAmiPSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db88add6-0a84-47ae-f43a-c1fce482bc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated model with L1 and L2 regularization applied has been saved as 'l1.h5'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/l1.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckc-NeF4j5EA",
        "outputId": "1ea5d45e-6342-410b-f60e-12df5d2800e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "47/47 [==============================] - 3s 5ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.91      0.68       624\n",
            "           1       0.88      0.46      0.61       875\n",
            "\n",
            "    accuracy                           0.65      1499\n",
            "   macro avg       0.71      0.69      0.65      1499\n",
            "weighted avg       0.74      0.65      0.64      1499\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.6504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EWC**"
      ],
      "metadata": {
        "id": "t7hqnyn3l9cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# Define the function to compute the EWC penalty\n",
        "def ewc_penalty(model, precision_matrices):\n",
        "    ewc_loss = 0\n",
        "    initial_weights = model.get_weights()\n",
        "    for j, var in enumerate(model.trainable_variables):\n",
        "        ewc_loss += tf.reduce_sum(precision_matrices[j] * tf.square(var - initial_weights[j]))\n",
        "    return ewc_loss\n",
        "\n",
        "\n",
        "def train_with_ewc(model, task_B_train, precision_matrices, epochs=6):\n",
        "    for epoch in range(epochs):\n",
        "        for batch_data in task_B_train:\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, preds))\n",
        "                ewc_loss = ewc_penalty(model, precision_matrices)\n",
        "                total_loss = loss + ewc_loss\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        task_set_shuffled = task_set.sample(frac=1)  # Shuffle the dataset\n",
        "        for start in range(0, len(task_set_shuffled), batch_size):\n",
        "            batch_data = task_set_shuffled[start:start+batch_size]\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                ll = tf.nn.log_softmax(preds)\n",
        "            ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "            for j, g in enumerate(ll_grads):\n",
        "                precision_matrices[j] += tf.reduce_mean(tf.square(g), axis=0) / num_batches\n",
        "\n",
        "    return precision_matrices\n",
        "\n",
        "\n",
        "# Load the pre-trained model\n",
        "pretrained_model_path = '/content/drive/MyDrive/Depression/Model/l1.h5'\n",
        "pretrained_model = tf.keras.models.load_model(pretrained_model_path)\n",
        "\n",
        "# Load dataset A\n",
        "task_A_train_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv'\n",
        "task_A_train_data = pd.read_csv(task_A_train_path)\n",
        "\n",
        "# Load dataset B\n",
        "task_B_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "task_B_train_data = pd.read_csv(task_B_train_path)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset A\n",
        "precision_matrices_A = compute_precision_matrices(pretrained_model, task_A_train_data, num_batches=100)\n",
        "\n",
        "# Shuffle dataset B\n",
        "task_B_train_data_shuffled = task_B_train_data.sample(frac=1)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset B\n",
        "precision_matrices_B = compute_precision_matrices(pretrained_model, task_B_train_data_shuffled, num_batches=100)\n",
        "\n",
        "# Fine-tune the model on dataset B with EWC\n",
        "train_with_ewc(pretrained_model, [task_B_train_data_shuffled], precision_matrices_B, epochs=5)\n",
        "\n",
        "# Save the model\n",
        "pretrained_model.save('/content/drive/MyDrive/Depression/Model/ewc_model.h5')\n",
        "print(\"Model trained with EWC has been saved as 'ewc_model.h5'\")\n"
      ],
      "metadata": {
        "id": "yxnTZ_HviPUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ec9eb9-bb54-4f33-92c1-555dc24473db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained with EWC has been saved as 'ewc_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/ewc_model.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "kAnePHwViPW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68d9447-0a2c-421d-a4ac-c56a00511c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       624\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "    accuracy                           0.42      1499\n",
            "   macro avg       0.21      0.50      0.29      1499\n",
            "weighted avg       0.17      0.42      0.24      1499\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import inspect  # Add this import\n",
        "\n",
        "# Load the dataset\n",
        "task_A_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv'\n",
        "task_A_train_data = pd.read_csv(task_A_train_path)\n",
        "\n",
        "# Print information about the dataset\n",
        "print(\"Information about the dataset:\")\n",
        "print(task_A_train_data.info())\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(task_A_train_data.head())\n",
        "\n",
        "# Print the compute_precision_matrices function definition\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    print(\"\\ncompute_precision_matrices function definition:\")\n",
        "    print(inspect.getsource(compute_precision_matrices))\n",
        "\n",
        "# Call the compute_precision_matrices function\n",
        "compute_precision_matrices(pretrained_model, task_A_train_data, num_batches=100)\n"
      ],
      "metadata": {
        "id": "qU7-4sAoiPY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a93b127-29f9-47ea-de1a-3cb396e10592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 790 entries, 0 to 789\n",
            "Columns: 514 entries, name to feature_511\n",
            "dtypes: float64(512), int64(1), object(1)\n",
            "memory usage: 3.1+ MB\n",
            "None\n",
            "\n",
            "First few rows of the dataset:\n",
            "                        name  Class  feature_0  feature_1  feature_2  \\\n",
            "0  Reading_depressed_151.wav      1 -18.882141  -8.257141  10.337055   \n",
            "1  Reading_depressed_152.wav      1 -22.440567  -7.324803  12.972845   \n",
            "2  Reading_depressed_153.wav      1 -24.144209   5.473886   6.452698   \n",
            "3  Reading_depressed_154.wav      1 -23.029322   8.752594   7.679756   \n",
            "4  Reading_depressed_155.wav      1 -20.563568  10.359172  13.883033   \n",
            "\n",
            "   feature_3  feature_4  feature_5  feature_6  feature_7  ...  feature_502  \\\n",
            "0  14.460301   0.022575 -12.631349 -19.638437  -1.236870  ...   -21.927385   \n",
            "1  12.255430   0.514035 -12.671404 -18.478327  -6.821404  ...   -25.670532   \n",
            "2   4.345125  -0.957478  -4.620906 -21.235149  14.255901  ...   -25.407789   \n",
            "3  14.045033  11.053968  -0.903941 -20.941151   8.330535  ...   -22.408459   \n",
            "4   7.843898  -4.275321   6.790169 -21.909451   7.129304  ...   -21.428715   \n",
            "\n",
            "   feature_503  feature_504  feature_505  feature_506  feature_507  \\\n",
            "0    12.426567    13.281584    -3.958668   -11.950349     9.544119   \n",
            "1    12.210644    17.206093    -4.002068    -8.980949    13.693151   \n",
            "2    10.383853    22.392843    -3.204525     2.591347    13.081939   \n",
            "3    10.678526   -13.742749    -3.827003    -7.590631     7.630740   \n",
            "4     5.815386     8.922497    -3.357304     3.575392     9.365634   \n",
            "\n",
            "   feature_508  feature_509  feature_510  feature_511  \n",
            "0     7.214572    11.497910    11.368524   -16.950394  \n",
            "1    13.203887    13.222167    14.136799   -13.861715  \n",
            "2     7.549160    -4.044655    10.796481   -18.457823  \n",
            "3     8.914572     1.137929    10.400751   -17.049408  \n",
            "4    10.166909    -1.377069    10.213140   -15.670437  \n",
            "\n",
            "[5 rows x 514 columns]\n",
            "\n",
            "compute_precision_matrices function definition:\n",
            "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
            "    print(\"\\ncompute_precision_matrices function definition:\")\n",
            "    print(inspect.getsource(compute_precision_matrices))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **extraas**"
      ],
      "metadata": {
        "id": "xSUN-jO1ifXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the model from the .h5 file\n",
        "model_path = \"/content/drive/MyDrive/latest_CNN.h5\"\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Display the architecture and parameters of the loaded model\n",
        "loaded_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbKl2DC8iPjB",
        "outputId": "911412b5-137e-4fd8-c94a-264b3d554a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 510, 32)           128       \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 255, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 253, 64)           6208      \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 126, 64)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 8064)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               1032320   \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1038785 (3.96 MB)\n",
            "Trainable params: 1038785 (3.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "dataset_path = \"/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv\"  # Update with your dataset path\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Assuming the features are all columns except the label column\n",
        "features = dataset.drop(columns=['Class'])\n",
        "\n",
        "# Get the shape of the features\n",
        "input_shape = features.shape[1:]  # Exclude the first dimension which represents the number of samples\n",
        "\n",
        "print(\"Input shape:\", input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OD3qq40C1zF",
        "outputId": "f6b377d0-7068-48c3-bb6f-a0b1b176892f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (513,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your CSV file\n",
        "csv_file_path = \"/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "dataframe = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Get the shape of the DataFrame\n",
        "shape = dataframe.shape\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(\"Shape of the DataFrame:\", shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCyBnwN1iPlD",
        "outputId": "ffeb23fd-bc39-487b-afb1-70feca3fd7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the DataFrame: (12547, 514)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EWC-2 Testing**"
      ],
      "metadata": {
        "id": "_NSQ3Xty1kXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# Define the function to compute the EWC penalty\n",
        "def ewc_penalty(model, precision_matrices):\n",
        "    ewc_loss = 0\n",
        "    initial_weights = model.get_weights()\n",
        "    for j, var in enumerate(model.trainable_variables):\n",
        "        ewc_loss += tf.reduce_sum(precision_matrices[j] * tf.square(var - initial_weights[j]))\n",
        "    return ewc_loss\n",
        "\n",
        "def train_with_ewc(model, task_B_train, precision_matrices, epochs=6):\n",
        "    for epoch in range(epochs):\n",
        "        for batch_data in task_B_train:\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, preds))\n",
        "                ewc_loss = ewc_penalty(model, precision_matrices)\n",
        "                total_loss = loss + ewc_loss\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        task_set_shuffled = task_set.sample(frac=1)  # Shuffle the dataset\n",
        "        for start in range(0, len(task_set_shuffled), batch_size):\n",
        "            batch_data = task_set_shuffled[start:start+batch_size]\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                ll = tf.nn.log_softmax(preds)\n",
        "            ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "            for j, g in enumerate(ll_grads):\n",
        "                precision_matrices[j] += tf.reduce_mean(tf.square(g), axis=0) / num_batches\n",
        "\n",
        "    return precision_matrices\n",
        "\n",
        "\n",
        "# Load the pre-trained model\n",
        "pretrained_model_path = '/content/drive/MyDrive/Depression/Model/l1.h5'\n",
        "pretrained_model = tf.keras.models.load_model(pretrained_model_path)\n",
        "\n",
        "# Load dataset A (Interview)\n",
        "train_csv_path_A = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "test_csv_path_A = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "task_A_train_data = pd.read_csv(train_csv_path_A)\n",
        "task_A_test_data = pd.read_csv(test_csv_path_A)\n",
        "\n",
        "# Load dataset B (Reading)\n",
        "train_csv_path_B = '/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv'\n",
        "test_csv_path_B = '/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv'\n",
        "task_B_train_data = pd.read_csv(train_csv_path_B)\n",
        "task_B_test_data = pd.read_csv(test_csv_path_B)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset A\n",
        "precision_matrices_A = compute_precision_matrices(pretrained_model, task_A_train_data, num_batches=100)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset B\n",
        "precision_matrices_B = compute_precision_matrices(pretrained_model, task_B_train_data, num_batches=100)\n",
        "\n",
        "# Fine-tune the model on dataset B with EWC\n",
        "train_with_ewc(pretrained_model, [task_B_train_data], precision_matrices_B, epochs=5)\n",
        "\n",
        "# Save the model\n",
        "pretrained_model.save('/content/drive/MyDrive/Depression/Model/ewc_model_2.h5')\n",
        "print(\"Model trained with EWC has been saved as 'ewc_model.h5'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-pRchnv1khS",
        "outputId": "18d92156-9493-462d-d7d9-e32c33636884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained with EWC has been saved as 'ewc_model.h5'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/ewc_model_2.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOY5oreD1ktU",
        "outputId": "64a50ffc-302b-43fe-8a77-228df51893e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      1.00      0.58       102\n",
            "           1       0.00      0.00      0.00       150\n",
            "\n",
            "    accuracy                           0.40       252\n",
            "   macro avg       0.20      0.50      0.29       252\n",
            "weighted avg       0.16      0.40      0.23       252\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.4048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrI2YeMg1kv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **corrections**"
      ],
      "metadata": {
        "id": "lUm5SZsQ7FL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Function to load and preprocess test data\n",
        "def load_and_preprocess_test_data(test_data_path):\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    return X_test_scaled, y_test\n",
        "\n",
        "# Function to evaluate model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(\"Model Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nAccuracy for Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Evaluate on first test dataset\n",
        "test_data_path_1 = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "X_test_1, y_test_1 = load_and_preprocess_test_data(test_data_path_1)\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "\n",
        "# Evaluate on second test dataset\n",
        "test_data_path_2 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_2, y_test_2 = load_and_preprocess_test_data(test_data_path_2)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Fine-tuning the model on Interview dataset\n",
        "interview_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "interview_train_data = pd.read_csv(interview_train_path)\n",
        "X_interview = interview_train_data.drop(columns=['name', 'Class']).values\n",
        "y_interview = interview_train_data['Class'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_interview_scaled = scaler.fit_transform(X_interview)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X_interview_scaled_reshaped = X_interview_scaled.reshape((X_interview_scaled.shape[0], X_interview_scaled.shape[1], 1))\n",
        "\n",
        "# Compile model with smaller learning rate\n",
        "loaded_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on Interview dataset\n",
        "loaded_model.fit(X_interview_scaled_reshaped, y_interview, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/finetuned_model.h5')\n",
        "\n",
        "# Evaluate the fine-tuned model on test datasets\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yygD8_Q7G5V",
        "outputId": "fd64b63a-0dd8-4728-d339-a3c49b625cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 4s 6ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.7522\n",
            "47/47 [==============================] - 0s 6ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4323\n",
            "Epoch 1/10\n",
            "85/85 [==============================] - 10s 11ms/step - loss: 1.8358 - accuracy: 0.5312 - val_loss: 0.9220 - val_accuracy: 0.3769\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.8041 - accuracy: 0.5449 - val_loss: 0.7724 - val_accuracy: 0.3739\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6902 - accuracy: 0.5687 - val_loss: 0.6965 - val_accuracy: 0.4585\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6481 - accuracy: 0.6351 - val_loss: 0.6275 - val_accuracy: 0.6098\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6154 - accuracy: 0.6741 - val_loss: 0.6005 - val_accuracy: 0.6677\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5889 - accuracy: 0.7008 - val_loss: 0.5697 - val_accuracy: 0.7211\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5505 - accuracy: 0.7290 - val_loss: 0.5298 - val_accuracy: 0.7671\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5279 - accuracy: 0.7461 - val_loss: 0.5027 - val_accuracy: 0.7849\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4996 - accuracy: 0.7598 - val_loss: 0.4653 - val_accuracy: 0.8160\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4694 - accuracy: 0.7814 - val_loss: 0.4216 - val_accuracy: 0.8412\n",
            " 1/77 [..............................] - ETA: 5s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.65      0.72      1897\n",
            "           1       0.28      0.47      0.35       549\n",
            "\n",
            "    accuracy                           0.61      2446\n",
            "   macro avg       0.54      0.56      0.54      2446\n",
            "weighted avg       0.69      0.61      0.64      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6112\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.84      0.69       624\n",
            "           1       0.83      0.57      0.68       875\n",
            "\n",
            "    accuracy                           0.68      1499\n",
            "   macro avg       0.71      0.70      0.68      1499\n",
            "weighted avg       0.73      0.68      0.68      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to load and preprocess test data\n",
        "def load_and_preprocess_test_data(test_data_path):\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    return X_test_scaled, y_test\n",
        "\n",
        "# Function to evaluate model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(\"Model Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nAccuracy for Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Evaluate on first test dataset\n",
        "test_data_path_1 = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "X_test_1, y_test_1 = load_and_preprocess_test_data(test_data_path_1)\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "\n",
        "# Evaluate on second test dataset\n",
        "test_data_path_2 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_2, y_test_2 = load_and_preprocess_test_data(test_data_path_2)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Fine-tuning the model on Interview dataset\n",
        "interview_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "interview_train_data = pd.read_csv(interview_train_path)\n",
        "X_interview = interview_train_data.drop(columns=['name', 'Class']).values\n",
        "y_interview = interview_train_data['Class'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_interview_scaled = scaler.fit_transform(X_interview)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X_interview_scaled_reshaped = X_interview_scaled.reshape((X_interview_scaled.shape[0], X_interview_scaled.shape[1], 1))\n",
        "\n",
        "# Compile model with smaller learning rate\n",
        "loaded_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on Interview dataset\n",
        "loaded_model.fit(X_interview_scaled_reshaped, y_interview, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/finetuned_model.h5')\n",
        "\n",
        "# Evaluate the fine-tuned model on test datasets\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Applying L1 and L2 Regularization\n",
        "l1_value = 0.1\n",
        "l2_value = 0.1\n",
        "\n",
        "for layer in loaded_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv1D) or isinstance(layer, tf.keras.layers.Dense):\n",
        "        layer.kernel_regularizer = tf.keras.regularizers.l1_l2(l1=l1_value, l2=l2_value)\n",
        "\n",
        "# Save the model with regularization applied\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/l1_regularized_model.h5')\n",
        "print(\"Model with L1 and L2 regularization applied has been saved.\")\n",
        "\n",
        "# Load the test data for L1 regularized model\n",
        "test_data_path_l1 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_l1, y_test_l1 = load_and_preprocess_test_data(test_data_path_l1)\n",
        "\n",
        "# Evaluate the L1 regularized model on test data\n",
        "evaluate_model(loaded_model, X_test_l1, y_test_l1)\n",
        "\n",
        "# Elastic Weight Consolidation (EWC)\n",
        "def ewc_penalty(model, precision_matrices):\n",
        "    ewc_loss = 0\n",
        "    initial_weights = model.get_weights()\n",
        "    for j, var in enumerate(model.trainable_variables):\n",
        "        ewc_loss += tf.reduce_sum(precision_matrices[j] * tf.square(var - initial_weights[j]))\n",
        "    return ewc_loss\n",
        "\n",
        "def train_with_ewc(model, task_B_train, precision_matrices, epochs=6):\n",
        "    for epoch in range(epochs):\n",
        "        for batch_data in task_B_train:\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, preds))\n",
        "                ewc_loss = ewc_penalty(model, precision_matrices)\n",
        "                total_loss = loss + ewc_loss\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        task_set_shuffled = task_set.sample(frac=1)  # Shuffle the dataset\n",
        "        for start in range(0, len(task_set_shuffled), batch_size):\n",
        "            batch_data = task_set_shuffled[start:start+batch_size]\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                ll = tf.nn.log_softmax(preds)\n",
        "            ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "            for j, g in enumerate(ll_grads):\n",
        "                precision_matrices[j] += tf.reduce_mean(tf.square(g), axis=0) / num_batches\n",
        "\n",
        "    return precision_matrices\n",
        "\n",
        "# Load the pre-trained model\n",
        "pretrained_model_path = '/content/drive/MyDrive/Depression/Model/l1_regularized_model.h5'\n",
        "pretrained_model = tf.keras.models.load_model(pretrained_model_path)\n",
        "\n",
        "# Load dataset A\n",
        "task_A_train_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv'\n",
        "task_A_train_data = pd.read_csv(task_A_train_path)\n",
        "\n",
        "# Load dataset B\n",
        "task_B_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "task_B_train_data = pd.read_csv(task_B_train_path)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset A\n",
        "precision_matrices_A = compute_precision_matrices(pretrained_model, task_A_train_data, num_batches=100)\n",
        "\n",
        "# Shuffle dataset B\n",
        "task_B_train_data_shuffled = task_B_train_data.sample(frac=1)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset B\n",
        "precision_matrices_B = compute_precision_matrices(pretrained_model, task_B_train_data_shuffled, num_batches=100)\n",
        "\n",
        "# Fine-tune the model on dataset B with EWC\n",
        "train_with_ewc(pretrained_model, [task_B_train_data_shuffled], precision_matrices_B, epochs=5)\n",
        "\n",
        "# Save the model\n",
        "pretrained_model.save('/content/drive/MyDrive/Depression/Model/ewc_model.h5')\n",
        "print(\"Model trained with EWC has been saved as 'ewc_model.h5'\")\n",
        "\n",
        "# Load the saved EWC model\n",
        "ewc_model_save_path = '/content/drive/MyDrive/Depression/Model/ewc_model.h5'\n",
        "ewc_loaded_model = load_model(ewc_model_save_path)\n",
        "print(\"EWC Model loaded successfully.\")\n",
        "\n",
        "# Evaluate the EWC model on test data\n",
        "evaluate_model(ewc_loaded_model, X_test_l1, y_test_l1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c71Sq6228dXd",
        "outputId": "b674486b-6f4a-45cd-f123-debc6618f2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.7522\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4323\n",
            "Epoch 1/10\n",
            "85/85 [==============================] - 4s 8ms/step - loss: 1.8743 - accuracy: 0.5256 - val_loss: 0.8948 - val_accuracy: 0.3769\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.7740 - accuracy: 0.5520 - val_loss: 0.7807 - val_accuracy: 0.3234\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5683 - val_loss: 0.7336 - val_accuracy: 0.3501\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.6678 - accuracy: 0.5798 - val_loss: 0.7128 - val_accuracy: 0.3843\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.6385 - accuracy: 0.6158 - val_loss: 0.6656 - val_accuracy: 0.4881\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.6157 - accuracy: 0.6370 - val_loss: 0.6246 - val_accuracy: 0.5742\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.5923 - accuracy: 0.6682 - val_loss: 0.6054 - val_accuracy: 0.6068\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 1s 9ms/step - loss: 0.5686 - accuracy: 0.6730 - val_loss: 0.5629 - val_accuracy: 0.6736\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5451 - accuracy: 0.7042 - val_loss: 0.5510 - val_accuracy: 0.6751\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5176 - accuracy: 0.7223 - val_loss: 0.5007 - val_accuracy: 0.7240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.56      0.66      1897\n",
            "           1       0.26      0.52      0.34       549\n",
            "\n",
            "    accuracy                           0.55      2446\n",
            "   macro avg       0.53      0.54      0.50      2446\n",
            "weighted avg       0.68      0.55      0.59      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.5507\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.71      0.62       624\n",
            "           1       0.74      0.60      0.66       875\n",
            "\n",
            "    accuracy                           0.64      1499\n",
            "   macro avg       0.65      0.65      0.64      1499\n",
            "weighted avg       0.66      0.64      0.65      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6431\n",
            "Model with L1 and L2 regularization applied has been saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.71      0.62       624\n",
            "           1       0.74      0.60      0.66       875\n",
            "\n",
            "    accuracy                           0.64      1499\n",
            "   macro avg       0.65      0.65      0.64      1499\n",
            "weighted avg       0.66      0.64      0.65      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7e735059a830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7e735059a830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained with EWC has been saved as 'ewc_model.h5'\n",
            "EWC Model loaded successfully.\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       624\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "    accuracy                           0.42      1499\n",
            "   macro avg       0.21      0.50      0.29      1499\n",
            "weighted avg       0.17      0.42      0.24      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to load and preprocess test data\n",
        "def load_and_preprocess_test_data(test_data_path):\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    return X_test_scaled, y_test\n",
        "\n",
        "# Function to evaluate model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(\"Model Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nAccuracy for Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Evaluate on first test dataset\n",
        "test_data_path_1 = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "X_test_1, y_test_1 = load_and_preprocess_test_data(test_data_path_1)\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "\n",
        "# Evaluate on second test dataset\n",
        "test_data_path_2 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_2, y_test_2 = load_and_preprocess_test_data(test_data_path_2)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Fine-tuning the model on Interview dataset\n",
        "interview_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "interview_train_data = pd.read_csv(interview_train_path)\n",
        "X_interview = interview_train_data.drop(columns=['name', 'Class']).values\n",
        "y_interview = interview_train_data['Class'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_interview_scaled = scaler.fit_transform(X_interview)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X_interview_scaled_reshaped = X_interview_scaled.reshape((X_interview_scaled.shape[0], X_interview_scaled.shape[1], 1))\n",
        "\n",
        "# Compile model with smaller learning rate\n",
        "loaded_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on Interview dataset\n",
        "loaded_model.fit(X_interview_scaled_reshaped, y_interview, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/finetuned_model.h5')\n",
        "\n",
        "# Evaluate the fine-tuned model on test datasets\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Applying L1 and L2 Regularization\n",
        "l1_value = 0.1\n",
        "l2_value = 0.1\n",
        "\n",
        "for layer in loaded_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv1D) or isinstance(layer, tf.keras.layers.Dense):\n",
        "        layer.kernel_regularizer = tf.keras.regularizers.l1_l2(l1=l1_value, l2=l2_value)\n",
        "\n",
        "# Save the model with regularization applied\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/l1_regularized_model.h5')\n",
        "print(\"Model with L1 and L2 regularization applied has been saved.\")\n",
        "\n",
        "# Load the test data for L1 regularized model\n",
        "test_data_path_l1 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_l1, y_test_l1 = load_and_preprocess_test_data(test_data_path_l1)\n",
        "\n",
        "# Evaluate the L1 regularized model on test data\n",
        "evaluate_model(loaded_model, X_test_l1, y_test_l1)\n",
        "\n",
        "# Elastic Weight Consolidation (EWC)\n",
        "def ewc_penalty(model, precision_matrices):\n",
        "    ewc_loss = 0\n",
        "    initial_weights = model.get_weights()\n",
        "    for j, var in enumerate(model.trainable_variables):\n",
        "        ewc_loss += tf.reduce_sum(precision_matrices[j] * tf.square(var - initial_weights[j]))\n",
        "    return ewc_loss\n",
        "\n",
        "def train_with_ewc(model, task_B_train, precision_matrices, epochs=6):\n",
        "    for epoch in range(epochs):\n",
        "        for batch_data in task_B_train:\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, preds))\n",
        "                ewc_loss = ewc_penalty(model, precision_matrices)\n",
        "                total_loss = loss + ewc_loss\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        task_set_shuffled = task_set.sample(frac=1)  # Shuffle the dataset\n",
        "        for start in range(0, len(task_set_shuffled), batch_size):\n",
        "            batch_data = task_set_shuffled[start:start+batch_size]\n",
        "            imgs = batch_data.iloc[:, 1:-1].to_numpy()  # Exclude 'name' and 'Class' columns\n",
        "            labels = batch_data['Class'].to_numpy()\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                ll = tf.nn.log_softmax(preds)\n",
        "            ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "            for j, g in enumerate(ll_grads):\n",
        "                precision_matrices[j] += tf.reduce_mean(tf.square(g), axis=0) / num_batches\n",
        "\n",
        "    return precision_matrices\n",
        "\n",
        "# Load the pre-trained model\n",
        "pretrained_model_path = '/content/drive/MyDrive/Depression/Model/l1_regularized_model.h5'\n",
        "pretrained_model = tf.keras.models.load_model(pretrained_model_path)\n",
        "\n",
        "# Load dataset A\n",
        "task_A_train_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv'\n",
        "task_A_train_data = pd.read_csv(task_A_train_path)\n",
        "\n",
        "# Load dataset B\n",
        "task_B_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "task_B_train_data = pd.read_csv(task_B_train_path)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset A\n",
        "precision_matrices_A = compute_precision_matrices(pretrained_model, task_A_train_data, num_batches=100)\n",
        "\n",
        "# Shuffle dataset B\n",
        "task_B_train_data_shuffled = task_B_train_data.sample(frac=1)\n",
        "\n",
        "# Compute Fisher information matrix and precision matrices for dataset B\n",
        "precision_matrices_B = compute_precision_matrices(pretrained_model, task_B_train_data_shuffled, num_batches=100)\n",
        "\n",
        "# Fine-tune the model on dataset B with EWC\n",
        "train_with_ewc(pretrained_model, [task_B_train_data_shuffled], precision_matrices_B, epochs=5)\n",
        "\n",
        "# Save the model\n",
        "pretrained_model.save('/content/drive/MyDrive/Depression/Model/ewc_model.h5')\n",
        "print(\"Model trained with EWC has been saved as 'ewc_model.h5'\")\n",
        "\n",
        "# Load the saved EWC model\n",
        "ewc_model_save_path = '/content/drive/MyDrive/Depression/Model/ewc_model.h5'\n",
        "ewc_loaded_model = load_model(ewc_model_save_path)\n",
        "print(\"EWC Model loaded successfully.\")\n",
        "\n",
        "# Evaluate the EWC model on test data\n",
        "evaluate_model(ewc_loaded_model, X_test_l1, y_test_l1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71PgEg9BJ6xI",
        "outputId": "6d82cb2e-b1df-48b2-b468-93b217fc506f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 4s 9ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.7522\n",
            "47/47 [==============================] - 0s 7ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4323\n",
            "Epoch 1/10\n",
            "85/85 [==============================] - 11s 21ms/step - loss: 1.9029 - accuracy: 0.5171 - val_loss: 0.8769 - val_accuracy: 0.4347\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.7857 - accuracy: 0.5549 - val_loss: 0.7437 - val_accuracy: 0.4125\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 1s 11ms/step - loss: 0.6841 - accuracy: 0.5820 - val_loss: 0.6826 - val_accuracy: 0.4777\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.6569 - accuracy: 0.6050 - val_loss: 0.6393 - val_accuracy: 0.5638\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.6244 - accuracy: 0.6566 - val_loss: 0.6081 - val_accuracy: 0.6217\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5872 - accuracy: 0.7112 - val_loss: 0.5693 - val_accuracy: 0.6884\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5535 - accuracy: 0.7402 - val_loss: 0.4893 - val_accuracy: 0.8012\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5318 - accuracy: 0.7387 - val_loss: 0.4500 - val_accuracy: 0.8264\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7687 - val_loss: 0.4796 - val_accuracy: 0.7804\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4721 - accuracy: 0.7803 - val_loss: 0.4297 - val_accuracy: 0.8205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.63      0.71      1897\n",
            "           1       0.28      0.50      0.36       549\n",
            "\n",
            "    accuracy                           0.60      2446\n",
            "   macro avg       0.55      0.56      0.53      2446\n",
            "weighted avg       0.69      0.60      0.63      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.5989\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.82      0.68       624\n",
            "           1       0.82      0.59      0.68       875\n",
            "\n",
            "    accuracy                           0.68      1499\n",
            "   macro avg       0.70      0.70      0.68      1499\n",
            "weighted avg       0.72      0.68      0.68      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with L1 and L2 regularization applied has been saved.\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.82      0.68       624\n",
            "           1       0.82      0.59      0.68       875\n",
            "\n",
            "    accuracy                           0.68      1499\n",
            "   macro avg       0.70      0.70      0.68      1499\n",
            "weighted avg       0.72      0.68      0.68      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f3d08436b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f3d08436b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained with EWC has been saved as 'ewc_model.h5'\n",
            "EWC Model loaded successfully.\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       624\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "    accuracy                           0.42      1499\n",
            "   macro avg       0.21      0.50      0.29      1499\n",
            "weighted avg       0.17      0.42      0.24      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the evaluate function\n",
        "def evaluate(model, X, y):\n",
        "    y_pred = np.argmax(model.predict(X), axis=1)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Load and preprocess test data function\n",
        "def load_and_preprocess_test_data(test_data_path):\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    return X_test_scaled, y_test\n",
        "\n",
        "# Function to evaluate model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(\"Model Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nAccuracy for Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Define EWC loss function\n",
        "def ewc_loss(labels, preds, model, F, theta_A):\n",
        "    loss_b = model.loss(labels, preds)\n",
        "    penalty = compute_elastic_penalty(F, model.trainable_variables, theta_A)\n",
        "    return loss_b + penalty\n",
        "\n",
        "# Define function to compute precision matrices\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    task_set = task_set.repeat()\n",
        "    precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "    for i, (imgs, labels) in enumerate(task_set.take(num_batches)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = model(imgs)\n",
        "            ll = tf.nn.log_softmax(preds)\n",
        "        ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "        for i, g in enumerate(ll_grads):\n",
        "            precision_matrices[i] += tf.math.reduce_mean(g ** 2, axis=0) / num_batches\n",
        "\n",
        "    return precision_matrices\n",
        "\n",
        "# Define function to compute elastic penalty\n",
        "def compute_elastic_penalty(F, theta, theta_A, alpha=25):\n",
        "    penalty = 0\n",
        "    for i, theta_i in enumerate(theta):\n",
        "        _penalty = tf.math.reduce_sum(F[i] * (theta_i - theta_A[i]) ** 2)\n",
        "        penalty += _penalty\n",
        "    return 0.5 * alpha * penalty\n",
        "\n",
        "# Define function to train with EWC\n",
        "def train_with_ewc(model, task_A_set, task_B_set, task_A_test, task_B_test, epochs=3):\n",
        "    # First we're going to fit to task A and retain a copy of parameters trained on Task A\n",
        "    model.fit(task_A_set, epochs=epochs)\n",
        "    theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
        "    # We'll only compute Fisher once, you can do it whenever\n",
        "    F = compute_precision_matrices(model, task_A_set, num_batches=1000)\n",
        "\n",
        "    print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, task_A_test)))\n",
        "\n",
        "    # Now we set up the training loop for task B with EWC\n",
        "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
        "\n",
        "    for epoch in range(epochs * 3):\n",
        "        accuracy.reset_states()\n",
        "        loss.reset_states()\n",
        "\n",
        "        for batch, (imgs, labels) in enumerate(task_B_set):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Make the predictions\n",
        "                preds = model(imgs)\n",
        "                # Compute EWC loss\n",
        "                total_loss = ewc_loss(labels, preds, model, F, theta_A)\n",
        "            # Compute the gradients of model's trainable parameters wrt total loss\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            # Update the model with gradients\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            # Report updated loss and accuracy\n",
        "            accuracy.update_state(labels, preds)\n",
        "            loss.update_state(labels, preds)\n",
        "            print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
        "                epoch + 1, batch + 1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
        "            )\n",
        "        print(\"\")\n",
        "\n",
        "    print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, task_B_test)))\n",
        "    print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, task_A_test)))\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Evaluate on first test dataset\n",
        "test_data_path_1 = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "X_test_1, y_test_1 = load_and_preprocess_test_data(test_data_path_1)\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "\n",
        "# Evaluate on second test dataset\n",
        "test_data_path_2 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_2, y_test_2 = load_and_preprocess_test_data(test_data_path_2)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Fine-tuning the model on Interview dataset\n",
        "interview_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "interview_train_data = pd.read_csv(interview_train_path)\n",
        "X_interview = interview_train_data.drop(columns=['name', 'Class']).values\n",
        "y_interview = interview_train_data['Class'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_interview_scaled = scaler.fit_transform(X_interview)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X_interview_scaled_reshaped = X_interview_scaled.reshape((X_interview_scaled.shape[0], X_interview_scaled.shape[1], 1))\n",
        "\n",
        "# Compile model with smaller learning rate\n",
        "loaded_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on Interview dataset\n",
        "loaded_model.fit(X_interview_scaled_reshaped, y_interview, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/finetuned_model.h5')\n",
        "\n",
        "# Evaluate the fine-tuned model on test datasets\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Define EWC model\n",
        "ewc_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(5)\n",
        "])\n",
        "\n",
        "ewc_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy')\n",
        "\n",
        "# Call train_with_ewc function with appropriate datasets\n",
        "# train_with_ewc(ewc_model, task_A_train, task_B_train, task_A_test, task_B_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2t7uFsHNhJE",
        "outputId": "82f53e03-0172-45c7-b432-11a71cd6b0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.7522\n",
            "47/47 [==============================] - 0s 4ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4323\n",
            "Epoch 1/10\n",
            "85/85 [==============================] - 6s 12ms/step - loss: 1.8209 - accuracy: 0.5308 - val_loss: 0.9257 - val_accuracy: 0.3680\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.7667 - accuracy: 0.5579 - val_loss: 0.7589 - val_accuracy: 0.3887\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6876 - accuracy: 0.5754 - val_loss: 0.7167 - val_accuracy: 0.4154\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6530 - accuracy: 0.6050 - val_loss: 0.6146 - val_accuracy: 0.6395\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6212 - accuracy: 0.6670 - val_loss: 0.5803 - val_accuracy: 0.7166\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5815 - accuracy: 0.7008 - val_loss: 0.5686 - val_accuracy: 0.7062\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5523 - accuracy: 0.7212 - val_loss: 0.5453 - val_accuracy: 0.7196\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5176 - accuracy: 0.7457 - val_loss: 0.4957 - val_accuracy: 0.7849\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4856 - accuracy: 0.7639 - val_loss: 0.4643 - val_accuracy: 0.8012\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4558 - accuracy: 0.7862 - val_loss: 0.4536 - val_accuracy: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.62      0.71      1897\n",
            "           1       0.29      0.52      0.37       549\n",
            "\n",
            "    accuracy                           0.60      2446\n",
            "   macro avg       0.55      0.57      0.54      2446\n",
            "weighted avg       0.70      0.60      0.63      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.5998\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.81      0.69       624\n",
            "           1       0.82      0.61      0.70       875\n",
            "\n",
            "    accuracy                           0.70      1499\n",
            "   macro avg       0.71      0.71      0.70      1499\n",
            "weighted avg       0.73      0.70      0.70      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the evaluate function\n",
        "def evaluate(model, X, y):\n",
        "    y_pred = np.argmax(model.predict(X), axis=1)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Load and preprocess test data function\n",
        "def load_and_preprocess_test_data(test_data_path):\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    return X_test_scaled, y_test\n",
        "\n",
        "# Function to evaluate model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(\"Model Classification Report for Testing Set:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nAccuracy for Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Define L2 penalty function\n",
        "def l2_penalty(theta, theta_A):\n",
        "    penalty = 0\n",
        "    for i, theta_i in enumerate(theta):\n",
        "        _penalty = tf.math.reduce_sum((theta_i - theta_A[i]) ** 2)\n",
        "        penalty += _penalty\n",
        "    return 0.5 * penalty\n",
        "\n",
        "# Define function to train with L2 regularization\n",
        "def train_with_l2(model, task_A_train, task_B_train, X_test_A, y_test_A, X_test_B, y_test_B, epochs=6):\n",
        "    model.fit(task_A_train, epochs=epochs)\n",
        "    theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
        "\n",
        "    print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, X_test_A, y_test_A)))\n",
        "\n",
        "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        accuracy.reset_states()\n",
        "        loss.reset_states()\n",
        "        for batch, (imgs, labels) in enumerate(task_B_train):\n",
        "            with tf.GradientTape() as tape:\n",
        "                preds = model(imgs)\n",
        "                total_loss = model.loss(labels, preds) + l2_penalty(model.trainable_variables, theta_A)\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            accuracy.update_state(labels, preds)\n",
        "            loss.update_state(labels, preds)\n",
        "            print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f} - Training on Task B\".format(\n",
        "                epoch + 1, batch + 1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
        "            )\n",
        "        print(\"\")\n",
        "\n",
        "    print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, X_test_B, y_test_B)))\n",
        "    print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, X_test_A, y_test_A)))\n",
        "\n",
        "# Define function to compute precision matrices\n",
        "def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
        "    task_set = task_set.repeat()\n",
        "    precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
        "\n",
        "    for i, (imgs, labels) in enumerate(task_set.take(num_batches)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = model(imgs)\n",
        "            ll = tf.nn.log_softmax(preds)\n",
        "        ll_grads = tape.gradient(ll, model.trainable_variables)\n",
        "        for i, g in enumerate(ll_grads):\n",
        "            precision_matrices[i] += tf.math.reduce_mean(g ** 2, axis=0) / num_batches\n",
        "\n",
        "    return precision_matrices\n",
        "\n",
        "# Define function to compute elastic penalty\n",
        "def compute_elastic_penalty(F, theta, theta_A, alpha=25):\n",
        "    penalty = 0\n",
        "    for i, theta_i in enumerate(theta):\n",
        "        _penalty = tf.math.reduce_sum(F[i] * (theta_i - theta_A[i]) ** 2)\n",
        "        penalty += _penalty\n",
        "    return 0.5 * alpha * penalty\n",
        "\n",
        "# Define EWC loss function\n",
        "def ewc_loss(labels, preds, model, F, theta_A):\n",
        "    loss_b = model.loss(labels, preds)\n",
        "    penalty = compute_elastic_penalty(F, model.trainable_variables, theta_A)\n",
        "    return loss_b + penalty\n",
        "\n",
        "# Define function to train with EWC\n",
        "def train_with_ewc(model, task_A_set, task_B_set, X_test_A, y_test_A, X_test_B, y_test_B, epochs=3):\n",
        "    # First we're going to fit to task A and retain a copy of parameters trained on Task A\n",
        "    model.fit(task_A_set, epochs=epochs)\n",
        "    theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
        "    # We'll only compute Fisher once, you can do it whenever\n",
        "    F = compute_precision_matrices(model, task_A_set, num_batches=1000)\n",
        "\n",
        "    print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, X_test_A, y_test_A)))\n",
        "\n",
        "    # Now we set up the training loop for task B with EWC\n",
        "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
        "\n",
        "    for epoch in range(epochs * 3):\n",
        "        accuracy.reset_states()\n",
        "        loss.reset_states()\n",
        "\n",
        "        for batch, (imgs, labels) in enumerate(task_B_set):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Make the predictions\n",
        "                preds = model(imgs)\n",
        "                # Compute EWC loss\n",
        "                total_loss = ewc_loss(labels, preds, model, F, theta_A)\n",
        "            # Compute the gradients of model's trainable parameters wrt total loss\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            # Update the model with gradients\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            # Report updated loss and accuracy\n",
        "            accuracy.update_state(labels, preds)\n",
        "            loss.update_state(labels, preds)\n",
        "            print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f} - Training on Task B\".format(\n",
        "                epoch + 1, batch + 1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
        "            )\n",
        "        print(\"\")\n",
        "\n",
        "    print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, X_test_B, y_test_B)))\n",
        "    print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, X_test_A, y_test_A)))\n",
        "\n",
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Model/latest_CNN.h5'\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Evaluate on first test dataset\n",
        "test_data_path_1 = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "X_test_1, y_test_1 = load_and_preprocess_test_data(test_data_path_1)\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "\n",
        "# Evaluate on second test dataset\n",
        "test_data_path_2 = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "X_test_2, y_test_2 = load_and_preprocess_test_data(test_data_path_2)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Fine-tuning the model on Interview dataset\n",
        "interview_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "interview_train_data = pd.read_csv(interview_train_path)\n",
        "X_interview = interview_train_data.drop(columns=['name', 'Class']).values\n",
        "y_interview = interview_train_data['Class'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_interview_scaled = scaler.fit_transform(X_interview)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X_interview_scaled_reshaped = X_interview_scaled.reshape((X_interview_scaled.shape[0], X_interview_scaled.shape[1], 1))\n",
        "\n",
        "# Compile model with smaller learning rate\n",
        "loaded_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on Interview dataset\n",
        "loaded_model.fit(X_interview_scaled_reshaped, y_interview, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "loaded_model.save('/content/drive/MyDrive/Depression/Model/finetuned_model.h5')\n",
        "\n",
        "# Evaluate the fine-tuned model on test datasets\n",
        "evaluate_model(loaded_model, X_test_1, y_test_1)\n",
        "evaluate_model(loaded_model, X_test_2, y_test_2)\n",
        "\n",
        "# Load dataset A\n",
        "task_A_train_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv'\n",
        "task_A_train_data = pd.read_csv(task_A_train_path)\n",
        "\n",
        "# Load dataset B\n",
        "task_B_train_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "task_B_train_data = pd.read_csv(task_B_train_path)\n",
        "\n",
        "# Call train_with_l2 function with appropriate datasets\n",
        "train_with_l2(l2_model, task_A_train_data, task_B_train_data, X_test_1, y_test_1, X_test_2, y_test_2)\n",
        "\n",
        "# Call train_with_ewc function with appropriate datasets\n",
        "train_with_ewc(ewc_model, task_A_train_data, task_B_train_data, X_test_1, y_test_1, X_test_2, y_test_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2y7BsryNPJBT",
        "outputId": "f6ac7083-1637-42c8-adc6-9d697bc89771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86      1897\n",
            "           1       0.26      0.06      0.10       549\n",
            "\n",
            "    accuracy                           0.75      2446\n",
            "   macro avg       0.52      0.51      0.48      2446\n",
            "weighted avg       0.66      0.75      0.69      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.7522\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.94      0.58       624\n",
            "           1       0.62      0.07      0.13       875\n",
            "\n",
            "    accuracy                           0.43      1499\n",
            "   macro avg       0.52      0.51      0.35      1499\n",
            "weighted avg       0.54      0.43      0.31      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.4323\n",
            "Epoch 1/10\n",
            "85/85 [==============================] - 3s 7ms/step - loss: 1.8404 - accuracy: 0.5126 - val_loss: 0.9123 - val_accuracy: 0.3769\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 0s 6ms/step - loss: 0.7804 - accuracy: 0.5535 - val_loss: 0.7646 - val_accuracy: 0.3798\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6851 - accuracy: 0.5783 - val_loss: 0.7177 - val_accuracy: 0.4154\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6569 - accuracy: 0.6062 - val_loss: 0.6503 - val_accuracy: 0.5252\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.6157 - accuracy: 0.6704 - val_loss: 0.5812 - val_accuracy: 0.6810\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5897 - accuracy: 0.6990 - val_loss: 0.5865 - val_accuracy: 0.6602\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5640 - accuracy: 0.7231 - val_loss: 0.5315 - val_accuracy: 0.7493\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5282 - accuracy: 0.7461 - val_loss: 0.4837 - val_accuracy: 0.7938\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.5032 - accuracy: 0.7517 - val_loss: 0.4508 - val_accuracy: 0.8323\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.7758 - val_loss: 0.4171 - val_accuracy: 0.8472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 2ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.65      0.72      1897\n",
            "           1       0.27      0.46      0.34       549\n",
            "\n",
            "    accuracy                           0.61      2446\n",
            "   macro avg       0.54      0.55      0.53      2446\n",
            "weighted avg       0.69      0.61      0.64      2446\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6075\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.85      0.70       624\n",
            "           1       0.84      0.58      0.69       875\n",
            "\n",
            "    accuracy                           0.69      1499\n",
            "   macro avg       0.72      0.71      0.69      1499\n",
            "weighted avg       0.74      0.69      0.69      1499\n",
            "\n",
            "\n",
            "Accuracy for Testing Set: 0.6918\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-db1fa4a2a109>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;31m# Call train_with_l2 function with appropriate datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m \u001b[0mtrain_with_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_A_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_B_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m# Call train_with_ewc function with appropriate datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-db1fa4a2a109>\u001b[0m in \u001b[0;36mtrain_with_l2\u001b[0;34m(model, task_A_train, task_B_train, X_test_A, y_test_A, X_test_B, y_test_B, epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Define function to train with L2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_with_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_A_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_B_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_A_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mtheta_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    }
  ]
}