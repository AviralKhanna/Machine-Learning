{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "hIzQE8QAYYE1",
        "B74sPqchYZgw",
        "Qhpa0KSEmzTD",
        "Qkw0xQHJpbVr",
        "j6p9vzH5AREl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading**"
      ],
      "metadata": {
        "id": "9J6I_taUYVCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB6uXYI6--ff",
        "outputId": "bcaa4831-ef9a-4858-f27e-1b5bce108606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "hIzQE8QAYYE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WITHOUT FUNCTIONAL API\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # Load training and testing data from CSV files\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv')\n",
        "\n",
        "# # Prepare features (X) and labels (y) for training and testing sets\n",
        "# X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "# y_train = train_data['Class'].values\n",
        "# X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "# y_test = test_data['Class'].values\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Reshape the data for RNN input\n",
        "# # RNNs require 3D input of shape [samples, timesteps, features]. Here, we'll treat each feature as a timestep.\n",
        "# # This might not be the optimal approach for every problem and depends on how your data is structured.\n",
        "# X_train_rnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "# X_test_rnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# # Initialize the RNN model\n",
        "# model_rnn = Sequential()\n",
        "# model_rnn.add(SimpleRNN(units=64, activation='relu', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2]), return_sequences=False))\n",
        "# model_rnn.add(Dense(50, activation='relu'))\n",
        "# model_rnn.add(Dropout(0.5))\n",
        "# model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile and train the RNN model\n",
        "# model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# history = model_rnn.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n",
        "\n",
        "# # Make predictions using the RNN for the testing set\n",
        "# y_test_pred_rnn = (model_rnn.predict(X_test_rnn) > 0.5).astype(int)\n",
        "\n",
        "# # Calculate and print the classification report and accuracy for the RNN model\n",
        "# print(\"RNN Model Classification Report for Testing Set:\")\n",
        "# print(classification_report(y_test, y_test_pred_rnn))\n",
        "# accuracy_test_rnn = accuracy_score(y_test, y_test_pred_rnn)\n",
        "# print(f\"\\nAccuracy for RNN Testing Set: {accuracy_test_rnn:.4f}\")"
      ],
      "metadata": {
        "id": "VynCINtDYXVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "ZwUH5NAEifmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load training and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv')\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(train_data, test_data):\n",
        "    # Prepare features and labels\n",
        "    X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "    y_train = train_data['Class'].values\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Reshape data for RNN: (samples, timesteps, features)\n",
        "    X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "    X_test_rnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "    return X_train_rnn, y_train, X_test_rnn, y_test\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_rnn, y_train, X_test_rnn, y_test = preprocess_data(train_data, test_data)\n",
        "\n",
        "# Function to create RNN model using the Functional API\n",
        "def create_functional_model(X_train_shape, rnn_units=64, dense_units=50, dropout_rate=0.5):\n",
        "    inputs = Input(shape=(X_train_shape[1], X_train_shape[2]))\n",
        "    x = SimpleRNN(units=rnn_units, activation='relu', return_sequences=False)(inputs)\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with the training data shape, RNN units, dense units, and dropout rate\n",
        "model = create_functional_model(X_train_rnn.shape, rnn_units=64, dense_units=50, dropout_rate=0.5)\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred = (model.predict(X_test_rnn) > 0.5).astype(int)\n",
        "print(\"Functional API Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"\\nAccuracy for Functional API Model Testing Set: {accuracy_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K1WSvGSgF2o",
        "outputId": "db98aff5-5af9-4be5-bf8e-64abfb0d4607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 64)                4224      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                3250      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7525 (29.39 KB)\n",
            "Trainable params: 7525 (29.39 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 13s 292ms/step - loss: 0.6938 - accuracy: 0.5278 - val_loss: 0.6862 - val_accuracy: 0.5794\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 8s 330ms/step - loss: 0.6706 - accuracy: 0.5696 - val_loss: 0.6810 - val_accuracy: 0.5714\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 7s 283ms/step - loss: 0.6593 - accuracy: 0.6038 - val_loss: 0.6826 - val_accuracy: 0.5714\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 8s 328ms/step - loss: 0.6500 - accuracy: 0.6241 - val_loss: 0.6824 - val_accuracy: 0.5873\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 8s 328ms/step - loss: 0.6212 - accuracy: 0.6506 - val_loss: 0.6634 - val_accuracy: 0.6508\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 7s 264ms/step - loss: 0.6020 - accuracy: 0.6709 - val_loss: 0.6589 - val_accuracy: 0.6389\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 8s 322ms/step - loss: 0.5497 - accuracy: 0.7316 - val_loss: 0.6645 - val_accuracy: 0.6468\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 7s 268ms/step - loss: 0.5073 - accuracy: 0.7684 - val_loss: 0.6497 - val_accuracy: 0.6230\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 9s 359ms/step - loss: 0.4721 - accuracy: 0.7873 - val_loss: 0.6123 - val_accuracy: 0.6706\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 9s 349ms/step - loss: 0.4204 - accuracy: 0.8190 - val_loss: 0.6092 - val_accuracy: 0.6825\n",
            "8/8 [==============================] - 1s 44ms/step\n",
            "Functional API Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.54      0.58       102\n",
            "           1       0.71      0.78      0.75       150\n",
            "\n",
            "    accuracy                           0.68       252\n",
            "   macro avg       0.67      0.66      0.66       252\n",
            "weighted avg       0.68      0.68      0.68       252\n",
            "\n",
            "\n",
            "Accuracy for Functional API Model Testing Set: 0.6825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "x9a7opsQgGce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "model.save('/content/drive/MyDrive/Depression/Android Dataset/Reading_RNN.h5')  # Save the model"
      ],
      "metadata": {
        "id": "KaI8_DucmJ4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd56bb80-5c7c-4eda-f8bf-8be8875a84a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the saved model\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the saved RNN model from your drive\n",
        "model_load_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_RNN.h5'  # Ensure this matches the path where you saved the model\n",
        "loaded_model = load_model(model_load_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load new test data\n",
        "new_test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv'  # Update this path\n",
        "new_test_data = pd.read_csv(new_test_data_path)\n",
        "\n",
        "# Assuming your test CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_new_test = new_test_data.drop(columns=['Class', 'name']).values\n",
        "y_new_test = new_test_data['Class'].values\n",
        "\n",
        "# Standardize the features using the same approach as was used for the training data\n",
        "scaler = StandardScaler()\n",
        "X_new_test_scaled = scaler.fit_transform(X_new_test)  # Note: In practice, use the same scaler as for the training data\n",
        "\n",
        "# Reshape the data for RNN input, assuming each feature is treated as a separate timestep\n",
        "X_new_test_scaled_rnn = X_new_test_scaled.reshape((X_new_test_scaled.shape[0], X_new_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions using the loaded RNN model for the new testing set\n",
        "y_new_test_pred_rnn = (loaded_model.predict(X_new_test_scaled_rnn) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy for the RNN model on the new test set\n",
        "print(\"RNN Model Classification Report for New Testing Set:\")\n",
        "print(classification_report(y_new_test, y_new_test_pred_rnn))\n",
        "accuracy_new_test_rnn = accuracy_score(y_new_test, y_new_test_pred_rnn)\n",
        "print(f\"\\nAccuracy for RNN on New Testing Set: {accuracy_new_test_rnn:.4f}\")\n"
      ],
      "metadata": {
        "id": "W8Mr8GfSmJ7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e83e5d-87bb-4e6b-84c9-23987b54f4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "8/8 [==============================] - 1s 43ms/step\n",
            "RNN Model Classification Report for New Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.56      0.59       102\n",
            "           1       0.72      0.77      0.75       150\n",
            "\n",
            "    accuracy                           0.69       252\n",
            "   macro avg       0.67      0.67      0.67       252\n",
            "weighted avg       0.68      0.69      0.68       252\n",
            "\n",
            "\n",
            "Accuracy for RNN on New Testing Set: 0.6865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "B74sPqchYZgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WITHOUT FUNCTIONAL API\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # Load training and testing data from CSV files\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv')\n",
        "\n",
        "# # Prepare features (X) and labels (y) for training and testing sets\n",
        "# X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "# y_train = train_data['Class'].values\n",
        "# X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "# y_test = test_data['Class'].values\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Reshape the data for CNN input\n",
        "# # Adjust the shape based on your actual feature extraction\n",
        "# X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "# X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# # Initialize the CNN model\n",
        "# model_cnn = Sequential()\n",
        "# model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
        "# model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "# model_cnn.add(Flatten())\n",
        "# model_cnn.add(Dense(50, activation='relu'))\n",
        "# model_cnn.add(Dropout(0.5))\n",
        "# model_cnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile and train the CNN model\n",
        "# model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
        "\n",
        "# # Make predictions using the CNN for the testing set\n",
        "# y_test_pred_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(int)\n",
        "\n",
        "# # Calculate and print the classification report and accuracy for the CNN model\n",
        "# print(\"CNN Model Classification Report for Testing Set:\")\n",
        "# print(classification_report(y_test, y_test_pred_cnn))\n",
        "# accuracy_test_cnn = accuracy_score(y_test, y_test_pred_cnn)\n",
        "# print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test_cnn:.4f}\")\n"
      ],
      "metadata": {
        "id": "_hoV4RJPZYEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "j8_2_BPDjovu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load training and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv')\n",
        "\n",
        "# Prepare features (X) and labels (y) for training and testing sets\n",
        "X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "y_train = train_data['Class'].values\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the data for CNN input\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "def create_functional_cnn_model(input_shape):\n",
        "    \"\"\"\n",
        "    Creates a CNN model for a binary classification task using the Functional API.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: The shape of the input data.\n",
        "\n",
        "    Returns:\n",
        "    - model: The CNN model defined using the Functional API.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the CNN model with the training data shape\n",
        "model_cnn = create_functional_cnn_model(X_train_cnn.shape[1:])\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model_cnn.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(int)\n",
        "print(\"Functional API CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_test_pred_cnn))\n",
        "accuracy_test_cnn = accuracy_score(y_test, y_test_pred_cnn)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test_cnn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l3qjLL6jo2_",
        "outputId": "0e3949b7-ca95-4394-a51a-b768b1af99cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 510, 64)           256       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 255, 64)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16320)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                816050    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 816357 (3.11 MB)\n",
            "Trainable params: 816357 (3.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 3s 18ms/step - loss: 0.6239 - accuracy: 0.6823 - val_loss: 0.5434 - val_accuracy: 0.7341\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8051 - val_loss: 0.4588 - val_accuracy: 0.7817\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3475 - accuracy: 0.8684 - val_loss: 0.3920 - val_accuracy: 0.8254\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2705 - accuracy: 0.8899 - val_loss: 0.3448 - val_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2171 - accuracy: 0.9114 - val_loss: 0.3721 - val_accuracy: 0.8214\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1930 - accuracy: 0.9316 - val_loss: 0.2855 - val_accuracy: 0.8651\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9519 - val_loss: 0.2759 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1011 - accuracy: 0.9646 - val_loss: 0.2996 - val_accuracy: 0.8492\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9633 - val_loss: 0.2107 - val_accuracy: 0.9206\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0970 - accuracy: 0.9671 - val_loss: 0.2264 - val_accuracy: 0.8849\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Functional API CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.75      0.84       102\n",
            "           1       0.85      0.97      0.91       150\n",
            "\n",
            "    accuracy                           0.88       252\n",
            "   macro avg       0.90      0.86      0.88       252\n",
            "weighted avg       0.89      0.88      0.88       252\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.8849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "model.save('/content/drive/MyDrive/Depression/Model/my_rnn_model.h5')  # Save the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDDhX3VD8DHz",
        "outputId": "55a67aa4-4261-40a5-eac2-e3efbeab8732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "M0F0Iz7bjo8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TESTING AFTER LOADING\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model from Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features (assuming this was done before training as well)\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "BswzqUYRlTuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153b0b82-ef99-4da2-c8d1-c97f9d318886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       102\n",
            "           1       0.93      0.87      0.90       150\n",
            "\n",
            "    accuracy                           0.88       252\n",
            "   macro avg       0.88      0.89      0.88       252\n",
            "weighted avg       0.89      0.88      0.89       252\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.8849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model from Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Reading_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features (assuming this was done before training as well)\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = loaded_model.evaluate(X_test_scaled, y_test, verbose=1)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "HEH6vFmskM1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb76418-079b-4e05-9175-15e5d9661171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.8849\n",
            "Test Loss: 0.2511\n",
            "Test Accuracy: 0.8849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interview**"
      ],
      "metadata": {
        "id": "hdFOqrZwmtAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "2kfqhd9BmtJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WITHOUT FUNCTIONAL API\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # Load training and testing data from CSV files\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/Interview_test.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/Interview_train.csv')\n",
        "\n",
        "# # Prepare features (X) and labels (y) for training and testing sets\n",
        "# X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "# y_train = train_data['Class'].values\n",
        "# X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "# y_test = test_data['Class'].values\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Reshape the data for CNN input\n",
        "# # Adjust the shape based on your actual feature extraction\n",
        "# X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "# X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# # Initialize the CNN model\n",
        "# model_cnn = Sequential()\n",
        "# model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
        "# model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "# model_cnn.add(Flatten())\n",
        "# model_cnn.add(Dense(50, activation='relu'))\n",
        "# model_cnn.add(Dropout(0.5))\n",
        "# model_cnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile and train the CNN model\n",
        "# model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
        "\n",
        "# # Make predictions using the CNN for the testing set\n",
        "# y_test_pred_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(int)\n",
        "\n",
        "# # Calculate and print the classification report and accuracy for the CNN model\n",
        "# print(\"CNN Model Classification Report for Testing Set:\")\n",
        "# print(classification_report(y_test, y_test_pred_cnn))\n",
        "# accuracy_test_cnn = accuracy_score(y_test, y_test_pred_cnn)\n",
        "# print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test_cnn:.4f}\")\n"
      ],
      "metadata": {
        "id": "xZ3A5Zt7mxEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "4i4_g7g3j-qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load training and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "# Prepare features (X) and labels (y) for training and testing sets\n",
        "X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "y_train = train_data['Class'].values\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the data for CNN input\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "def create_functional_cnn_model(input_shape):\n",
        "    \"\"\"\n",
        "    Creates a CNN model for a binary classification task using the Functional API.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: The shape of the input data.\n",
        "\n",
        "    Returns:\n",
        "    - model: The CNN model defined using the Functional API.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the CNN model with the training data shape\n",
        "model_cnn = create_functional_cnn_model(X_train_cnn.shape[1:])\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model_cnn.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(int)\n",
        "print(\"Functional API CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_test_pred_cnn))\n",
        "accuracy_test_cnn = accuracy_score(y_test, y_test_pred_cnn)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test_cnn:.4f}\")\n"
      ],
      "metadata": {
        "id": "iSLdWKI6j-vK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6a9e7b-f933-41a6-c4e3-09397e0632ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 510, 64)           256       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 255, 64)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16320)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                816050    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 816357 (3.11 MB)\n",
            "Trainable params: 816357 (3.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "106/106 [==============================] - 6s 8ms/step - loss: 0.4571 - accuracy: 0.7838 - val_loss: 0.6171 - val_accuracy: 0.7338\n",
            "Epoch 2/10\n",
            "106/106 [==============================] - 1s 6ms/step - loss: 0.2615 - accuracy: 0.8907 - val_loss: 0.8479 - val_accuracy: 0.7011\n",
            "Epoch 3/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.1843 - accuracy: 0.9317 - val_loss: 0.7288 - val_accuracy: 0.7338\n",
            "Epoch 4/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.1473 - accuracy: 0.9466 - val_loss: 0.6322 - val_accuracy: 0.7625\n",
            "Epoch 5/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.1174 - accuracy: 0.9534 - val_loss: 0.3910 - val_accuracy: 0.8312\n",
            "Epoch 6/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.1154 - accuracy: 0.9564 - val_loss: 0.5701 - val_accuracy: 0.8152\n",
            "Epoch 7/10\n",
            "106/106 [==============================] - 1s 6ms/step - loss: 0.1062 - accuracy: 0.9569 - val_loss: 0.6656 - val_accuracy: 0.7799\n",
            "Epoch 8/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.0759 - accuracy: 0.9718 - val_loss: 0.5073 - val_accuracy: 0.8172\n",
            "Epoch 9/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.0696 - accuracy: 0.9718 - val_loss: 0.7068 - val_accuracy: 0.7925\n",
            "Epoch 10/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.0737 - accuracy: 0.9730 - val_loss: 0.7201 - val_accuracy: 0.8025\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Functional API CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.98      0.81       624\n",
            "           1       0.98      0.68      0.80       875\n",
            "\n",
            "    accuracy                           0.80      1499\n",
            "   macro avg       0.83      0.83      0.80      1499\n",
            "weighted avg       0.86      0.80      0.80      1499\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.8025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "om5Aauqwj-yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "model.save('/content/drive/MyDrive/Depression/Android Dataset/Interview_CNN.h5')  # Save the model"
      ],
      "metadata": {
        "id": "Zs9H2cGRnDV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46ad462-22ec-47f0-ca8f-e8e66b2c40a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TESTING AFTER LOADING\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model from Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_CNN.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features (assuming this was done before training as well)\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "av7gtbnTnDZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b66606-10c7-4354-b633-7cda65d58bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "47/47 [==============================] - 2s 39ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.61       624\n",
            "           1       0.72      0.75      0.73       875\n",
            "\n",
            "    accuracy                           0.69      1499\n",
            "   macro avg       0.68      0.67      0.67      1499\n",
            "weighted avg       0.68      0.69      0.68      1499\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.6858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "Qhpa0KSEmzTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WITHOUT FUNCTIONAL API\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # Load training and testing data from CSV files\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/Interview_train.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/Interview_test.csv')\n",
        "\n",
        "# # Prepare features (X) and labels (y) for training and testing sets\n",
        "# X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "# y_train = train_data['Class'].values\n",
        "# X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "# y_test = test_data['Class'].values\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Reshape the data for RNN input\n",
        "# # RNNs require 3D input of shape [samples, timesteps, features]. Here, we'll treat each feature as a timestep.\n",
        "# # This might not be the optimal approach for every problem and depends on how your data is structured.\n",
        "# X_train_rnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "# X_test_rnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# # Initialize the RNN model\n",
        "# model_rnn = Sequential()\n",
        "# model_rnn.add(SimpleRNN(units=64, activation='relu', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2]), return_sequences=False))\n",
        "# model_rnn.add(Dense(50, activation='relu'))\n",
        "# model_rnn.add(Dropout(0.5))\n",
        "# model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile and train the RNN model\n",
        "# model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# history = model_rnn.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n",
        "\n",
        "# # Make predictions using the RNN for the testing set\n",
        "# y_test_pred_rnn = (model_rnn.predict(X_test_rnn) > 0.5).astype(int)\n",
        "\n",
        "# # Calculate and print the classification report and accuracy for the RNN model\n",
        "# print(\"RNN Model Classification Report for Testing Set:\")\n",
        "# print(classification_report(y_test, y_test_pred_rnn))\n",
        "# accuracy_test_rnn = accuracy_score(y_test, y_test_pred_rnn)\n",
        "# print(f\"\\nAccuracy for RNN Testing Set: {accuracy_test_rnn:.4f}\")\n"
      ],
      "metadata": {
        "id": "wsGCdCPFmzc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "fTbKUZUVnI7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load training and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(train_data, test_data):\n",
        "    # Prepare features and labels\n",
        "    X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "    y_train = train_data['Class'].values\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Reshape data for RNN: (samples, timesteps, features)\n",
        "    X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "    X_test_rnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "    return X_train_rnn, y_train, X_test_rnn, y_test\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_rnn, y_train, X_test_rnn, y_test = preprocess_data(train_data, test_data)\n",
        "\n",
        "# Function to create RNN model using the Functional API\n",
        "def create_functional_model(X_train_shape, rnn_units=64, dense_units=50, dropout_rate=0.5):\n",
        "    inputs = Input(shape=(X_train_shape[1], X_train_shape[2]))\n",
        "    x = SimpleRNN(units=rnn_units, activation='relu', return_sequences=False)(inputs)\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with the training data shape, RNN units, dense units, and dropout rate\n",
        "model = create_functional_model(X_train_rnn.shape, rnn_units=64, dense_units=50, dropout_rate=0.5)\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred = (model.predict(X_test_rnn) > 0.5).astype(int)\n",
        "print(\"Functional API Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"\\nAccuracy for Functional API Model Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79D__Pg1nJAJ",
        "outputId": "fb10d649-f9d5-48b0-97b4-fa0b6e97545a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 64)                4224      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                3250      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7525 (29.39 KB)\n",
            "Trainable params: 7525 (29.39 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "106/106 [==============================] - 33s 297ms/step - loss: 0.6300 - accuracy: 0.6467 - val_loss: 0.6975 - val_accuracy: 0.6204\n",
            "Epoch 2/10\n",
            "106/106 [==============================] - 35s 330ms/step - loss: 0.5690 - accuracy: 0.7052 - val_loss: 0.6693 - val_accuracy: 0.5891\n",
            "Epoch 3/10\n",
            "106/106 [==============================] - 30s 288ms/step - loss: 0.5337 - accuracy: 0.7328 - val_loss: 0.7103 - val_accuracy: 0.5991\n",
            "Epoch 4/10\n",
            "106/106 [==============================] - 31s 296ms/step - loss: 0.5054 - accuracy: 0.7426 - val_loss: 0.6757 - val_accuracy: 0.6391\n",
            "Epoch 5/10\n",
            "106/106 [==============================] - 32s 302ms/step - loss: 0.4784 - accuracy: 0.7672 - val_loss: 0.6858 - val_accuracy: 0.6338\n",
            "Epoch 6/10\n",
            "106/106 [==============================] - 30s 285ms/step - loss: 0.4510 - accuracy: 0.7865 - val_loss: 0.7092 - val_accuracy: 0.6391\n",
            "Epoch 7/10\n",
            "106/106 [==============================] - 32s 299ms/step - loss: 0.4341 - accuracy: 0.8005 - val_loss: 0.7081 - val_accuracy: 0.6311\n",
            "Epoch 8/10\n",
            "106/106 [==============================] - 30s 285ms/step - loss: 0.4166 - accuracy: 0.8094 - val_loss: 0.6834 - val_accuracy: 0.6471\n",
            "Epoch 9/10\n",
            "106/106 [==============================] - 31s 292ms/step - loss: 0.3998 - accuracy: 0.8034 - val_loss: 0.6838 - val_accuracy: 0.6604\n",
            "Epoch 10/10\n",
            "106/106 [==============================] - 32s 298ms/step - loss: 0.3869 - accuracy: 0.8236 - val_loss: 0.7019 - val_accuracy: 0.6598\n",
            "47/47 [==============================] - 2s 44ms/step\n",
            "Functional API Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.88      0.68       624\n",
            "           1       0.85      0.51      0.63       875\n",
            "\n",
            "    accuracy                           0.66      1499\n",
            "   macro avg       0.70      0.69      0.66      1499\n",
            "weighted avg       0.73      0.66      0.65      1499\n",
            "\n",
            "\n",
            "Accuracy for Functional API Model Testing Set: 0.6598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "3kr0NrPBnJF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "model.save('/content/drive/MyDrive/Depression/Android Dataset/Interview_RNN.h5')  # Save the model"
      ],
      "metadata": {
        "id": "w1ORsv8JnmN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad5f6ca-f62f-43d1-fcde-121f326f84cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the saved model\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the saved RNN model from your drive\n",
        "model_load_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_RNN.h5'  # Ensure this matches the path where you saved the model\n",
        "loaded_model = load_model(model_load_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load new test data\n",
        "new_test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update this path\n",
        "new_test_data = pd.read_csv(new_test_data_path)\n",
        "\n",
        "# Assuming your test CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_new_test = new_test_data.drop(columns=['Class', 'name']).values\n",
        "y_new_test = new_test_data['Class'].values\n",
        "\n",
        "# Standardize the features using the same approach as was used for the training data\n",
        "scaler = StandardScaler()\n",
        "X_new_test_scaled = scaler.fit_transform(X_new_test)  # Note: In practice, use the same scaler as for the training data\n",
        "\n",
        "# Reshape the data for RNN input, assuming each feature is treated as a separate timestep\n",
        "X_new_test_scaled_rnn = X_new_test_scaled.reshape((X_new_test_scaled.shape[0], X_new_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions using the loaded RNN model for the new testing set\n",
        "y_new_test_pred_rnn = (loaded_model.predict(X_new_test_scaled_rnn) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy for the RNN model on the new test set\n",
        "print(\"RNN Model Classification Report for New Testing Set:\")\n",
        "print(classification_report(y_new_test, y_new_test_pred_rnn))\n",
        "accuracy_new_test_rnn = accuracy_score(y_new_test, y_new_test_pred_rnn)\n",
        "print(f\"\\nAccuracy for RNN on New Testing Set: {accuracy_new_test_rnn:.4f}\")"
      ],
      "metadata": {
        "id": "_DwfYzjZnmQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81511bfc-50c4-4f0a-b1c0-0e343c016743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "47/47 [==============================] - 2s 38ms/step\n",
            "RNN Model Classification Report for New Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.90      0.68       624\n",
            "           1       0.87      0.46      0.60       875\n",
            "\n",
            "    accuracy                           0.65      1499\n",
            "   macro avg       0.71      0.68      0.64      1499\n",
            "weighted avg       0.73      0.65      0.63      1499\n",
            "\n",
            "\n",
            "Accuracy for RNN on New Testing Set: 0.6451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDAIC**"
      ],
      "metadata": {
        "id": "0WmU0fWKpXRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "X2Cikx9fpZyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install pandas numpy scikit-learn tensorflow\n",
        "# # 75%\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # Function to pad features to the next perfect square\n",
        "# def pad_features_to_square(features):\n",
        "#     num_features = features.shape[1]\n",
        "#     next_square = np.ceil(np.sqrt(num_features))**2\n",
        "#     padding_size = int(next_square - num_features)\n",
        "#     return np.pad(features, ((0, 0), (0, padding_size)), 'constant')\n",
        "\n",
        "# # Function to prepare data for CNN\n",
        "# def prepare_data_for_cnn(features, size):\n",
        "#     padded_features = pad_features_to_square(features)\n",
        "#     reshaped_features = padded_features.reshape((features.shape[0], size, size, 1))\n",
        "#     return reshaped_features\n",
        "\n",
        "\n",
        "# # Load data from the CSV files\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv')\n",
        "# validation_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/validation_labels.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "\n",
        "# # Drop rows with NaN values from all datasets\n",
        "# train_data.dropna(inplace=True)\n",
        "# validation_data.dropna(inplace=True)\n",
        "# test_data.dropna(inplace=True)\n",
        "\n",
        "# # Prepare features and labels\n",
        "# X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "# y_train = train_data['Class'].values\n",
        "# X_validation = validation_data.drop(columns=['Class', 'name']).values\n",
        "# y_validation = validation_data['Class'].values\n",
        "# X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "# y_test = test_data['Class'].values\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler().fit(np.vstack((X_train, X_validation, X_test)))\n",
        "# X_train_scaled = scaler.transform(X_train)\n",
        "# X_validation_scaled = scaler.transform(X_validation)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # Calculate size for reshaping based on training data\n",
        "# size = int(np.sqrt(pad_features_to_square(X_train_scaled).shape[1]))\n",
        "\n",
        "# # Prepare the data for CNN\n",
        "# X_train_cnn = prepare_data_for_cnn(X_train_scaled, size)\n",
        "# X_validation_cnn = prepare_data_for_cnn(X_validation_scaled, size)\n",
        "# X_test_cnn = prepare_data_for_cnn(X_test_scaled, size)\n",
        "\n",
        "# # CNN model definition with dropout to reduce overfitting\n",
        "# def create_cnn_model(input_shape):\n",
        "#     model = Sequential([\n",
        "#         Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Dropout(0.25),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Dropout(0.25),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(1, activation='sigmoid')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Initialize and train the CNN model\n",
        "# model = create_cnn_model((size, size, 1))\n",
        "# model.fit(X_train_cnn, y_train, epochs=10, batch_size=64, validation_data=(X_validation_cnn, y_validation))\n",
        "\n",
        "# # Evaluate the model on the test data\n",
        "# y_pred = (model.predict(X_test_cnn) > 0.5).astype(int)\n",
        "# print(\"Classification Report for Testing Set:\")\n",
        "# print(classification_report(y_test, y_pred))\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f\"Accuracy for Testing Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "dkD3Pl89pa5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "7qBp_o96kYYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load training, validation, and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv')  # Adjusted for training data\n",
        "validation_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/validation_labels.csv')  # New validation data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "# Prepare features (X) and labels (y) for training, validation, and testing sets\n",
        "X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "y_train = train_data['Class'].values\n",
        "\n",
        "X_validation = validation_data.drop(columns=['Class', 'name']).values\n",
        "y_validation = validation_data['Class'].values\n",
        "\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(np.vstack((X_train, X_validation, X_test)))  # Fit on all available data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_validation_scaled = scaler.transform(X_validation)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the data for CNN input\n",
        "X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "X_validation_cnn = X_validation_scaled.reshape((X_validation_scaled.shape[0], X_validation_scaled.shape[1], 1))\n",
        "X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "def create_functional_cnn_model(input_shape):\n",
        "    \"\"\"\n",
        "    Creates a CNN model for a binary classification task using the Functional API.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: The shape of the input data.\n",
        "\n",
        "    Returns:\n",
        "    - model: The CNN model defined using the Functional API.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the CNN model with the training data shape\n",
        "model_cnn = create_functional_cnn_model(X_train_cnn.shape[1:])\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model_cnn.summary()\n",
        "\n",
        "# Train the model with validation data\n",
        "history = model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_validation_cnn, y_validation))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(int)\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_test_pred_cnn))\n",
        "accuracy_test_cnn = accuracy_score(y_test, y_test_pred_cnn)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test_cnn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIcwRzK9kYpE",
        "outputId": "ff921f49-b77f-44ae-f456-4e79ad0033ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 510, 64)           256       \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 255, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 16320)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 50)                816050    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 816357 (3.11 MB)\n",
            "Trainable params: 816357 (3.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "393/393 [==============================] - 4s 8ms/step - loss: nan - accuracy: 0.8102 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 2/10\n",
            "393/393 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 3/10\n",
            "393/393 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 4/10\n",
            "393/393 [==============================] - 4s 9ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 5/10\n",
            "393/393 [==============================] - 4s 10ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 6/10\n",
            "393/393 [==============================] - 3s 8ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 7/10\n",
            "393/393 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 8/10\n",
            "393/393 [==============================] - 3s 7ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 9/10\n",
            "393/393 [==============================] - 2s 5ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "Epoch 10/10\n",
            "393/393 [==============================] - 2s 5ms/step - loss: nan - accuracy: 0.8120 - val_loss: nan - val_accuracy: 0.8105\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      1.00      0.87      1897\n",
            "           1       0.00      0.00      0.00       549\n",
            "\n",
            "    accuracy                           0.78      2446\n",
            "   macro avg       0.39      0.50      0.44      2446\n",
            "weighted avg       0.60      0.78      0.68      2446\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.7756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "111# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "qaooQHWZkYr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924c09ef-b2e2-4623-c66f-e92e5d1f81bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "model_cnn.save('/content/drive/MyDrive/Depression/EDAIC Dataset/MODEL.h5')  # Save the model\n"
      ],
      "metadata": {
        "id": "gldgKBm3pbEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46625b44-2b4c-49f2-d6aa-4505ca07fe28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TESTING AFTER LOADING\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model from Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/MODEL.h5'  # Update with your model's path\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'  # Update with your test data's path\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Assuming your CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "y_test = test_data['Class'].values\n",
        "\n",
        "# Scale the features (assuming this was done before training as well)\n",
        "scaler = StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "# Reshape the data if your model expects a specific input shape (example for CNN)\n",
        "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = (loaded_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy\n",
        "print(\"CNN Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy for CNN Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "zB-Y2gQdpbH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb30dcd7-8acd-484f-a2e6-e662d6da5b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "CNN Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      1.00      0.87      1897\n",
            "           1       0.00      0.00      0.00       549\n",
            "\n",
            "    accuracy                           0.78      2446\n",
            "   macro avg       0.39      0.50      0.44      2446\n",
            "weighted avg       0.60      0.78      0.68      2446\n",
            "\n",
            "\n",
            "Accuracy for CNN Testing Set: 0.7756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "Qkw0xQHJpbVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.model_selection import KFold\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # Load data from the CSV files\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/training_labels.csv')\n",
        "# validation_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/validation_labels.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "# # Drop rows with NaN values from all datasets\n",
        "# train_data.dropna(inplace=True)\n",
        "# validation_data.dropna(inplace=True)\n",
        "# test_data.dropna(inplace=True)\n",
        "\n",
        "# # Prepare features and labels\n",
        "# X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "# y_train = train_data['Class'].values\n",
        "# X_validation = validation_data.drop(columns=['Class', 'name']).values\n",
        "# y_validation = validation_data['Class'].values\n",
        "# X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "# y_test = test_data['Class'].values\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler().fit(np.vstack((X_train, X_validation, X_test)))\n",
        "# X_train_scaled = scaler.transform(X_train)\n",
        "# X_validation_scaled = scaler.transform(X_validation)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # No need to pad features to a perfect square for RNNs\n",
        "# # Reshape data for RNN input\n",
        "# num_features = X_train_scaled.shape[1]  # Number of features for each sample\n",
        "# X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], num_features, 1))\n",
        "# X_validation_rnn = X_validation_scaled.reshape((X_validation_scaled.shape[0], num_features, 1))\n",
        "# X_test_rnn = X_test_scaled.reshape((X_test_scaled.shape[0], num_features, 1))\n",
        "\n",
        "# # Function to create an RNN (LSTM) model\n",
        "# def create_rnn_model(input_shape):\n",
        "#     model = Sequential([\n",
        "#         LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "#         Dropout(0.25),\n",
        "#         LSTM(32),\n",
        "#         Dropout(0.25),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(1, activation='sigmoid')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # 5-fold cross-validation configuration\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# # Perform 5-fold cross-validation\n",
        "# fold_no = 1\n",
        "# for train_index, val_index in kf.split(X_train_rnn, y_train):\n",
        "#     # Split data into training and validation for the current fold\n",
        "#     X_train_fold, X_val_fold = X_train_rnn[train_index], X_train_rnn[val_index]\n",
        "#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "#     # Create a new RNN model for the current fold\n",
        "#     model = create_rnn_model((num_features, 1))\n",
        "\n",
        "#     # Train the model\n",
        "#     print(f'Training on fold {fold_no}...')\n",
        "#     model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=64, validation_data=(X_val_fold, y_val_fold))\n",
        "\n",
        "#     fold_no += 1\n",
        "\n",
        "# # Optionally, retrain the model on the full training dataset or select the best model from folds\n",
        "# # This example proceeds with the model from the last fold for simplicity\n",
        "\n",
        "# # Evaluate the model on the test data\n",
        "# y_pred = (model.predict(X_test_rnn) > 0.5).astype(int)\n",
        "# print(\"Classification Report for Testing Set:\")\n",
        "# print(classification_report(y_test, y_pred))\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f\"Accuracy for Testing Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "3_MhYG2MpdXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "bbwf9pHvo519"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load training, validation, and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Reading_train.csv')\n",
        "validation_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/validation_labels.csv')  # Adjusted for validation data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(train_data, validation_data, test_data):\n",
        "    # Prepare features and labels for training data\n",
        "    X_train = train_data.drop(columns=['Class', 'name']).values\n",
        "    y_train = train_data['Class'].values\n",
        "\n",
        "    # Prepare features and labels for validation data\n",
        "    X_validation = validation_data.drop(columns=['Class', 'name']).values\n",
        "    y_validation = validation_data['Class'].values\n",
        "\n",
        "    # Prepare features and labels for testing data\n",
        "    X_test = test_data.drop(columns=['Class', 'name']).values\n",
        "    y_test = test_data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(np.vstack((X_train, X_validation, X_test)))  # Fit on all available data\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_validation_scaled = scaler.transform(X_validation)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Reshape data for RNN: (samples, timesteps, features)\n",
        "    X_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "    X_validation_rnn = X_validation_scaled.reshape((X_validation_scaled.shape[0], X_validation_scaled.shape[1], 1))\n",
        "    X_test_rnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "    return X_train_rnn, y_train, X_validation_rnn, y_validation, X_test_rnn, y_test\n",
        "\n",
        "# Preprocess the data including validation set\n",
        "X_train_rnn, y_train, X_validation_rnn, y_validation, X_test_rnn, y_test = preprocess_data(train_data, validation_data, test_data)\n",
        "\n",
        "# Function to create RNN model using the Functional API\n",
        "def create_functional_model(X_train_shape, rnn_units=64, dense_units=50, dropout_rate=0.5):\n",
        "    inputs = Input(shape=(X_train_shape[1], X_train_shape[2]))\n",
        "    x = SimpleRNN(units=rnn_units, activation='relu', return_sequences=False)(inputs)\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with the training data shape, RNN units, dense units, and dropout rate\n",
        "model = create_functional_model(X_train_rnn.shape, rnn_units=64, dense_units=50, dropout_rate=0.5)\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model with validation data\n",
        "history = model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_validation_rnn, y_validation))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred = (model.predict(X_test_rnn) > 0.5).astype(int)\n",
        "print(\"Functional API Model Classification Report for Testing Set:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"\\nAccuracy for Functional API Model Testing Set: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3B1djDno552",
        "outputId": "c554d40c-5b65-4157-f975-f4184159bd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 512, 1)]          0         \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 64)                4224      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 50)                3250      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7525 (29.39 KB)\n",
            "Trainable params: 7525 (29.39 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 16s 593ms/step - loss: 0.6939 - accuracy: 0.5114 - val_loss: 0.6644 - val_accuracy: 0.5688\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 11s 453ms/step - loss: 0.6756 - accuracy: 0.5646 - val_loss: 0.6592 - val_accuracy: 0.5668\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 13s 520ms/step - loss: 0.6623 - accuracy: 0.5899 - val_loss: 0.6634 - val_accuracy: 0.5651\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 11s 440ms/step - loss: 0.6532 - accuracy: 0.6190 - val_loss: 0.6786 - val_accuracy: 0.5621\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 11s 440ms/step - loss: 0.6348 - accuracy: 0.6228 - val_loss: 0.6836 - val_accuracy: 0.5701\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 10s 409ms/step - loss: 0.6246 - accuracy: 0.6405 - val_loss: 0.7055 - val_accuracy: 0.5524\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 11s 427ms/step - loss: 0.6080 - accuracy: 0.6646 - val_loss: 0.6962 - val_accuracy: 0.5775\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 13s 518ms/step - loss: 0.5729 - accuracy: 0.6937 - val_loss: 0.6978 - val_accuracy: 0.5922\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 13s 518ms/step - loss: 0.5471 - accuracy: 0.7392 - val_loss: 0.7090 - val_accuracy: 0.6096\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 11s 450ms/step - loss: 0.5194 - accuracy: 0.7380 - val_loss: 0.8011 - val_accuracy: 0.5403\n",
            "77/77 [==============================] - 3s 38ms/step\n",
            "Functional API Model Classification Report for Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.60      0.71      1897\n",
            "           1       0.34      0.71      0.46       549\n",
            "\n",
            "    accuracy                           0.62      2446\n",
            "   macro avg       0.61      0.65      0.58      2446\n",
            "weighted avg       0.76      0.62      0.65      2446\n",
            "\n",
            "\n",
            "Accuracy for Functional API Model Testing Set: 0.6231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "SCMs4IG5o58-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/EDIAC_RNN.h5')\n"
      ],
      "metadata": {
        "id": "4dum9KP9pdij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b54d49f-ae86-4771-b7b3-9038b5505eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the saved model\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the saved RNN model from your drive\n",
        "model_load_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/EDIAC_RNN.h5'  # Ensure this matches the path where you saved the model\n",
        "loaded_model = load_model(model_load_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load new test data\n",
        "new_test_data_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'  # Update this path\n",
        "new_test_data = pd.read_csv(new_test_data_path)\n",
        "\n",
        "# Assuming your test CSV has specific columns to drop (like 'name') and a 'Class' column for labels\n",
        "X_new_test = new_test_data.drop(columns=['Class', 'name']).values\n",
        "y_new_test = new_test_data['Class'].values\n",
        "\n",
        "# Standardize the features using the same approach as was used for the training data\n",
        "scaler = StandardScaler()\n",
        "X_new_test_scaled = scaler.fit_transform(X_new_test)  # Note: In practice, use the same scaler as for the training data\n",
        "\n",
        "# Reshape the data for RNN input, assuming each feature is treated as a separate timestep\n",
        "X_new_test_scaled_rnn = X_new_test_scaled.reshape((X_new_test_scaled.shape[0], X_new_test_scaled.shape[1], 1))\n",
        "\n",
        "# Make predictions using the loaded RNN model for the new testing set\n",
        "y_new_test_pred_rnn = (loaded_model.predict(X_new_test_scaled_rnn) > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the classification report and accuracy for the RNN model on the new test set\n",
        "print(\"RNN Model Classification Report for New Testing Set:\")\n",
        "print(classification_report(y_new_test, y_new_test_pred_rnn))\n",
        "accuracy_new_test_rnn = accuracy_score(y_new_test, y_new_test_pred_rnn)\n",
        "print(f\"\\nAccuracy for RNN on New Testing Set: {accuracy_new_test_rnn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro77LUvLpdmM",
        "outputId": "4a2e2d98-2d0c-4987-b0e3-9d7704d86580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "77/77 [==============================] - 3s 39ms/step\n",
            "RNN Model Classification Report for New Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.55      0.67      1897\n",
            "           1       0.31      0.71      0.44       549\n",
            "\n",
            "    accuracy                           0.59      2446\n",
            "   macro avg       0.59      0.63      0.56      2446\n",
            "weighted avg       0.74      0.59      0.62      2446\n",
            "\n",
            "\n",
            "Accuracy for RNN on New Testing Set: 0.5871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE-TUNING INTER DATASETS**"
      ],
      "metadata": {
        "id": "QE5nybtzAQhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# taking the weights/trained model on EDAIC-CNN and fine tune on Android interview train set"
      ],
      "metadata": {
        "id": "duXBPjVlAQt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the dataset\n",
        "interview_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "\n",
        "# Prepare the dataset by dropping the 'name' column and separating features and labels\n",
        "X = interview_df.drop(columns=['name', 'Class']).values\n",
        "y = interview_df['Class'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/EDAIC_CNN.h5')\n",
        "\n",
        "# Optionally, you can make some of the layers non-trainable if you wish\n",
        "# for layer in model.layers[:-n]: # Replace n with the number of layers you want to fine-tune\n",
        "#     layer.trainable = False\n",
        "\n",
        "# Compile the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fine-tune the model on the new dataset\n",
        "model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=10, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/Fine_tuned_EDAIC_CNN.h5')\n"
      ],
      "metadata": {
        "id": "E0SgWO4kDDaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc76301-02ff-4e58-eaf2-757dfe12cfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 13s 305ms/step - loss: 0.7194 - accuracy: 0.6580 - val_loss: 0.6245 - val_accuracy: 0.6500\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 11s 302ms/step - loss: 0.6047 - accuracy: 0.7014 - val_loss: 0.5781 - val_accuracy: 0.6733\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 11s 295ms/step - loss: 0.5564 - accuracy: 0.7289 - val_loss: 0.5574 - val_accuracy: 0.6967\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 11s 294ms/step - loss: 0.5358 - accuracy: 0.7339 - val_loss: 0.5461 - val_accuracy: 0.7000\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 12s 309ms/step - loss: 0.5109 - accuracy: 0.7515 - val_loss: 0.5467 - val_accuracy: 0.7067\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 11s 285ms/step - loss: 0.5022 - accuracy: 0.7548 - val_loss: 0.5439 - val_accuracy: 0.7100\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 10s 268ms/step - loss: 0.4861 - accuracy: 0.7581 - val_loss: 0.5434 - val_accuracy: 0.7033\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 11s 303ms/step - loss: 0.4746 - accuracy: 0.7631 - val_loss: 0.5497 - val_accuracy: 0.7033\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 12s 306ms/step - loss: 0.4619 - accuracy: 0.7756 - val_loss: 0.5444 - val_accuracy: 0.7033\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 12s 305ms/step - loss: 0.4595 - accuracy: 0.7815 - val_loss: 0.5434 - val_accuracy: 0.7067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **alllll**"
      ],
      "metadata": {
        "id": "7KNpZ92VQi-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the training and validation dataset\n",
        "interview_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "\n",
        "# Prepare the dataset by dropping the 'name' column and separating features and labels\n",
        "X = interview_df.drop(columns=['name', 'Class']).values\n",
        "y = interview_df['Class'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/EDAIC_CNN.h5')\n",
        "\n",
        "# Compile the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fine-tune the model on the new dataset\n",
        "model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=10, callbacks=[early_stopping])\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/testFine_tuned_EDAIC_CNN.h5')\n",
        "\n",
        "# Load and prepare the test dataset (Assuming you have a separate test set)\n",
        "# Replace 'Interview_test.csv' with your actual test dataset file if different\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')  # Update path if needed\n",
        "X_test = test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = test_df['Class'].values\n",
        "\n",
        "# Scale the features of the test set using the same scaler as for training\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/testFine_tuned_EDAIC_CNN.h5')\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = fine_tuned_model.predict(X_test_scaled)\n",
        "# Convert probabilities to class labels based on a threshold\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, predicted_classes)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4140amtKQj8A",
        "outputId": "e904bfa6-35a8-4bc6-9740-7e35f2fe7fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 13s 305ms/step - loss: 0.7055 - accuracy: 0.6731 - val_loss: 0.6238 - val_accuracy: 0.6567\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 11s 293ms/step - loss: 0.6107 - accuracy: 0.7089 - val_loss: 0.5744 - val_accuracy: 0.6800\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 11s 289ms/step - loss: 0.5530 - accuracy: 0.7356 - val_loss: 0.5557 - val_accuracy: 0.7000\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.5252 - accuracy: 0.7339 - val_loss: 0.5499 - val_accuracy: 0.7067\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 11s 287ms/step - loss: 0.5042 - accuracy: 0.7473 - val_loss: 0.5502 - val_accuracy: 0.7000\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 11s 286ms/step - loss: 0.4868 - accuracy: 0.7615 - val_loss: 0.5498 - val_accuracy: 0.7100\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 10s 254ms/step - loss: 0.4890 - accuracy: 0.7673 - val_loss: 0.5472 - val_accuracy: 0.7100\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 11s 288ms/step - loss: 0.4768 - accuracy: 0.7748 - val_loss: 0.5475 - val_accuracy: 0.7033\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 11s 295ms/step - loss: 0.4710 - accuracy: 0.7807 - val_loss: 0.5463 - val_accuracy: 0.7133\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 11s 290ms/step - loss: 0.4544 - accuracy: 0.7815 - val_loss: 0.5455 - val_accuracy: 0.7100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 2s 39ms/step\n",
            "Test Accuracy: 0.7798532354903269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With the android fine-tuned test it again on EDAIC test set"
      ],
      "metadata": {
        "id": "MspVNKsWIbQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/testFine_tuned_EDAIC_CNN.h5')\n",
        "\n",
        "# Load the EDAIC test dataset\n",
        "edaic_test_df = pd.read_csv('/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv')\n",
        "\n",
        "# Assuming the structure is similar to the interview dataset\n",
        "X_test = edaic_test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = edaic_test_df['Class'].values\n",
        "\n",
        "# Scale the features (use the same scaler parameters as used for the training data)\n",
        "# IMPORTANT: Fit the scaler on training data and then transform test data with it\n",
        "# For demonstration, we're assuming you have saved the scaler or its parameters\n",
        "scaler = StandardScaler().fit(X_train)  # X_train should be replaced with your training data or load the scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "\n",
        "# <60%\n"
      ],
      "metadata": {
        "id": "kUZSmiSwDDca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f252bd8-6dfb-44cc-a603-0ba330bf6dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 3s 41ms/step - loss: 0.7441 - accuracy: 0.7261\n",
            "Test Loss: 0.7440702319145203\n",
            "Test Accuracy: 0.7260833978652954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENSOR FLOW AVAILABLE"
      ],
      "metadata": {
        "id": "j6p9vzH5AREl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the dataset from your drive\n",
        "interview_df = pd.read_csv('/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv')\n",
        "\n",
        "# Prepare the dataset by dropping the 'name' column and separating features (X) and labels (y)\n",
        "X = interview_df.drop(columns=['name', 'Class']).values\n",
        "y = interview_df['Class'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features with StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Load the pre-trained CNN model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/EDAIC_CNN.h5')\n",
        "\n",
        "# If desired, set some layers to non-trainable (e.g., for feature extraction only)\n",
        "# Example: Freeze all but the last n layers\n",
        "# for layer in model.layers[:-n]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# Compile the model with a smaller learning rate for fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use EarlyStopping to halt training when validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fine-tune the model on the new dataset\n",
        "model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=100,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Save the fine-tuned model to your drive\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/Fine_tuned_EDAIC_CNN.h5')\n"
      ],
      "metadata": {
        "id": "cAa0HBriF5wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Assuming you've uploaded the fine-tuned model and the dataset to your Colab workspace\n",
        "model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/Fine_tuned_EDAIC_CNN.h5'\n",
        "test_data_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Load the EDAIC test dataset\n",
        "edaic_test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "# Prepare the dataset: extract features and labels, and scale the features\n",
        "X_test = edaic_test_df.drop(columns=['name', 'Class']).values\n",
        "y_test = edaic_test_df['Class'].values\n",
        "\n",
        "# Scale the features using the same scaler parameters as used for the training data\n",
        "# IMPORTANT: This assumes the scaler was fitted on the training data\n",
        "scaler = StandardScaler().fit(X_test)  # Normally, you should use the scaler fitted on the training data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "FExB9PdLF5y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e31292-7e17-4fbf-964e-8e5ff7229452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 4s 53ms/step - loss: 1.0930 - accuracy: 0.5025\n",
            "Test Loss: 1.0930414199829102\n",
            "Test Accuracy: 0.5024529695510864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without fine-tuning- taking the weights/trained model on EDAIC-CNN and test on Android interview train set\n"
      ],
      "metadata": {
        "id": "2L2g2JlGHLaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/EDAIC_CNN.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Load the Android dataset CSV that already contains the extracted features\n",
        "android_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "android_df = pd.read_csv(android_data_path)\n",
        "\n",
        "# Assuming your CSV has the features in columns named 'feature1', 'feature2', ..., 'featureN'\n",
        "# Adjust the column names according to your CSV file structure\n",
        "# If your features are not in separate columns but rather concatenated in one, you'll need to split them accordingly\n",
        "features_columns = [col for col in android_df.columns if col.startswith('feature')]\n",
        "X_test = android_df[features_columns].values\n",
        "\n",
        "# If you have labels in your CSV and they need to be tested against the model's predictions\n",
        "# Assuming the label is in a column named 'label'\n",
        "y_test = android_df['Class'].values\n",
        "\n",
        "# Reshape X_test if your model expects a specific input shape (e.g., adding a channel dimension for CNNs)\n",
        "# This reshape depends on your model's architecture\n",
        "# Example for adding a single channel dimension if needed\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Evaluate the model on the Android dataset\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1wcmPniHPlo",
        "outputId": "954e265b-d512-420d-ee88-547519990941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 2s 39ms/step - loss: 12.8446 - accuracy: 0.4183\n",
            "Test Loss: 12.844561576843262\n",
            "Test Accuracy: 0.41827884316444397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing EDAIC trained model on Android dataset"
      ],
      "metadata": {
        "id": "TanHhKMQpfpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Adjusted function to preprocess your data\n",
        "def preprocess_data(df):\n",
        "    # Selecting features (all columns except the first two) and labels (second column)\n",
        "    X = df.iloc[:, 2:].values  # Adjust if your features start from a different column\n",
        "    y = df.iloc[:, 1].values  # Labels are in the second column\n",
        "\n",
        "    # Standardizing features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# Load test data\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "\n",
        "test_data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Preprocess the data\n",
        "X_test, y_true = preprocess_data(test_data)\n",
        "\n",
        "# Load the CNN model\n",
        "model_file_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/MODEL.h5'  # Make sure to update this path to your model's location\n",
        "\n",
        "model = load_model(model_file_path)\n",
        "\n",
        "# Make predictions with the model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# # Assuming a binary classification task and predictions being probabilities,\n",
        "# # convert probabilities to binary outcomes based on a threshold (e.g., 0.5)\n",
        "# predictions_binary = (predictions > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_true, predictions_binary)\n",
        "print(f\"Model accuracy on the test set: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yjV0FHopjuE",
        "outputId": "67e29726-01e7-495a-9e96-eec5eb9179f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 3ms/step\n",
            "Model accuracy on the test set: 41.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now again training the model.h5 by android cnn"
      ],
      "metadata": {
        "id": "YQWbZ925tvNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to preprocess data (new data in this case)\n",
        "def preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    X = data.drop(columns=['Class', 'name']).values\n",
        "    y = data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Reshape the data for CNN input\n",
        "    X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "    return X_cnn, y\n",
        "\n",
        "# Load new training and testing data\n",
        "new_train_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'  # Update this path\n",
        "new_test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'    # Update this path\n",
        "\n",
        "X_train_new, y_train_new = preprocess_data(new_train_data_path)\n",
        "X_test_new, y_test_new = preprocess_data(new_test_data_path)\n",
        "\n",
        "# Load the existing model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/MODEL.h5')\n",
        "\n",
        "# Optionally, if you want to adjust the model's learning rate or other parameters:\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Further train the model on your new data\n",
        "model.fit(X_train_new, y_train_new, epochs=10, batch_size=32, validation_data=(X_test_new, y_test_new))\n",
        "\n",
        "# Evaluate the updated model on the new test set\n",
        "y_test_pred = (model.predict(X_test_new) > 0.5).astype(int)\n",
        "print(\"Updated Model Classification Report for New Testing Set:\")\n",
        "print(classification_report(y_test_new, y_test_pred))\n",
        "accuracy_test = accuracy_score(y_test_new, y_test_pred)\n",
        "print(f\"\\nUpdated Model Accuracy for New Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Save the updated model\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpcl_TWItvYs",
        "outputId": "6a4e0e46-0896-4785-8460-4b25611e3f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "106/106 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 2/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 3/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 4/10\n",
            "106/106 [==============================] - 1s 6ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 5/10\n",
            "106/106 [==============================] - 1s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 6/10\n",
            "106/106 [==============================] - 1s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 7/10\n",
            "106/106 [==============================] - 1s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 8/10\n",
            "106/106 [==============================] - 1s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 9/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 10/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Updated Model Classification Report for New Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       624\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "    accuracy                           0.42      1499\n",
            "   macro avg       0.21      0.50      0.29      1499\n",
            "weighted avg       0.17      0.42      0.24      1499\n",
            "\n",
            "\n",
            "Updated Model Accuracy for New Testing Set: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# testing the new updated model on ANDROID TEST\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Adjusted function to preprocess your data\n",
        "def preprocess_data(df):\n",
        "    # Selecting features (all columns except the first two) and labels (second column)\n",
        "    X = df.iloc[:, 2:].values  # Adjust if your features start from a different column\n",
        "    y = df.iloc[:, 1].values  # Labels are in the second column\n",
        "\n",
        "    # Standardizing features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# Load test data\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "\n",
        "test_data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Preprocess the data\n",
        "X_test, y_true = preprocess_data(test_data)\n",
        "\n",
        "# Load the CNN model\n",
        "model_file_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5'  # Make sure to update this path to your model's location\n",
        "\n",
        "model = load_model(model_file_path)\n",
        "\n",
        "# Make predictions with the model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Assuming a binary classification task and predictions being probabilities,\n",
        "# convert probabilities to binary outcomes based on a threshold (e.g., 0.5)\n",
        "predictions_binary = (predictions > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_true, predictions_binary)\n",
        "print(f\"Model accuracy on the test set: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLxHlXSvuZFn",
        "outputId": "e4b8c097-cf87-4237-9f98-c726592e3bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 2ms/step\n",
            "Model accuracy on the test set: 41.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bTFwNxdjxaU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Adjusted function to preprocess your data\n",
        "def preprocess_data(df):\n",
        "    # Selecting features (all columns except the first two) and labels (second column)\n",
        "    X = df.iloc[:, 2:].values  # Adjust if your features start from a different column\n",
        "    y = df.iloc[:, 1].values  # Labels are in the second column\n",
        "\n",
        "    # Standardizing features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# Load test data\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "test_data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Preprocess the data\n",
        "X_test, y_true = preprocess_data(test_data)\n",
        "\n",
        "# Load the CNN model\n",
        "model_file_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5'  # Make sure to update this path to your model's location\n",
        "model = load_model(model_file_path)\n",
        "\n",
        "# Make predictions with the model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to binary outcomes based on a threshold (e.g., 0.5)\n",
        "predictions_binary = (predictions > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_true, predictions_binary)\n",
        "print(f\"Model accuracy on the test set: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXgpe0ilxarD",
        "outputId": "258fdf60-0591-43ad-d4c3-4668d854387f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 2ms/step\n",
            "Model accuracy on the test set: 77.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HsFr4571xa06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finetuned\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "\n",
        "# Function to preprocess the Android-specific data\n",
        "def preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    X = data.drop(columns=['Class', 'name']).values  # Adjust these column names based on your dataset\n",
        "    y = data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Reshape the data for CNN input if necessary\n",
        "    X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "    return X_reshaped, y\n",
        "\n",
        "# Load the Android-specific training and testing data\n",
        "train_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'  # Update this path\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'    # Update this path\n",
        "X_train_android, y_train_android = preprocess_data(train_data_path)\n",
        "X_test_android, y_test_android = preprocess_data(test_data_path)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5')\n",
        "\n",
        "# Decide which layers to unfreeze for the fine-tuning process\n",
        "# Example: Unfreezing the last 3 layers. Adjust this based on your model's architecture.\n",
        "for layer in model.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model with a smaller learning rate for fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss=BinaryCrossentropy(), metrics=[Accuracy()])\n",
        "\n",
        "# Fine-tune the model on the new Android-specific data\n",
        "model.fit(X_train_android, y_train_android, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the fine-tuned model on the Android-specific test data\n",
        "loss, accuracy = model.evaluate(X_test_android, y_test_android)\n",
        "print(f\"Fine-tuned Model Loss: {loss}, Fine-tuned Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/finetuned.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V70UfhSPu75J",
        "outputId": "ab3c981d-8d67-4032-86d8-31eb6362fd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "85/85 [==============================] - 4s 19ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "85/85 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "85/85 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "85/85 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "85/85 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "47/47 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Fine-tuned Model Loss: nan, Fine-tuned Model Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nPqWbFx2wxGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to preprocess the test data\n",
        "def preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    X = data.drop(columns=['Class', 'name']).values  # Adjust these column names based on your dataset\n",
        "    y = data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Reshape the data for CNN input if necessary\n",
        "    X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "    return X_reshaped, y\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = load_model('/content/drive/MyDrive/Depression/EDAIC Dataset/finetuned.h5')\n",
        "\n",
        "# Load and preprocess the test data\n",
        "test_data_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'  # Update this path to your actual test.csv file path\n",
        "X_test, y_test = preprocess_data(test_data_path)\n",
        "\n",
        "# Make predictions with the fine-tuned model\n",
        "predictions = model.predict(X_test)\n",
        "predictions_binary = (predictions > 0.5).astype(\"int32\").flatten()  # Adjust threshold and method based on your problem\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, predictions_binary)\n",
        "print(f\"Accuracy of the fine-tuned model on test.csv: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dURWjRF0wxbn",
        "outputId": "49b25c6d-9d33-4eac-f164-56ea7f4bea5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 0s 3ms/step\n",
            "Accuracy of the fine-tuned model on test.csv: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without finetuning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Load the Android dataset CSV that already contains the extracted features\n",
        "android_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'\n",
        "android_df = pd.read_csv(android_data_path)\n",
        "\n",
        "# Assuming your CSV has the features in columns named 'feature1', 'feature2', ..., 'featureN'\n",
        "# Adjust the column names according to your CSV file structure\n",
        "# If your features are not in separate columns but rather concatenated in one, you'll need to split them accordingly\n",
        "features_columns = [col for col in android_df.columns if col.startswith('feature')]\n",
        "X_test = android_df[features_columns].values\n",
        "\n",
        "# If you have labels in your CSV and they need to be tested against the model's predictions\n",
        "# Assuming the label is in a column named 'label'\n",
        "y_test = android_df['Class'].values\n",
        "\n",
        "# Reshape X_test if your model expects a specific input shape (e.g., adding a channel dimension for CNNs)\n",
        "# This reshape depends on your model's architecture\n",
        "# Example for adding a single channel dimension if needed\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Evaluate the model on the Android dataset\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfODEplXvT9x",
        "outputId": "89f94712-85a7-45a5-fdf4-e15f311df8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.4163\n",
            "Test Loss: nan\n",
            "Test Accuracy: 0.41627752780914307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-gPMHNmxppV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to preprocess data (new data in this case)\n",
        "def preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    X = data.drop(columns=['Class', 'name']).values  # Adjust 'name' column if not applicable\n",
        "    y = data['Class'].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Reshape the data for CNN input\n",
        "    X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "    return X_cnn, y\n",
        "\n",
        "# Paths to your new training and testing data\n",
        "new_train_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'  # Update this path\n",
        "new_test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'    # Update this path\n",
        "\n",
        "# Load new training and testing data\n",
        "X_train_new, y_train_new = preprocess_data(new_train_data_path)\n",
        "X_test_new, y_test_new = preprocess_data(new_test_data_path)\n",
        "\n",
        "# Load the existing model\n",
        "model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/MODEL.h5'  # Update this path\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Optionally, adjust the model's learning rate or other parameters\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Further train the model on your new data\n",
        "model.fit(X_train_new, y_train_new, epochs=10, batch_size=32, validation_data=(X_test_new, y_test_new))\n",
        "\n",
        "# Evaluate the updated model on the new test set\n",
        "y_test_pred = (model.predict(X_test_new) > 0.5).astype(int)\n",
        "print(\"Updated Model Classification Report for New Testing Set:\")\n",
        "print(classification_report(y_test_new, y_test_pred))\n",
        "accuracy_test = accuracy_score(y_test_new, y_test_pred)\n",
        "print(f\"\\nUpdated Model Accuracy for New Testing Set: {accuracy_test:.4f}\")\n",
        "\n",
        "# Save the updated model\n",
        "updated_model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5'  # Update this path\n",
        "model.save(updated_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X1ZscAIxpxI",
        "outputId": "29b0cf71-d643-4b98-fad7-3d12c25fcbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "106/106 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 2/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 3/10\n",
            "106/106 [==============================] - 1s 6ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 4/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 5/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 6/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 7/10\n",
            "106/106 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 8/10\n",
            "106/106 [==============================] - 1s 6ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 9/10\n",
            "106/106 [==============================] - 1s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "Epoch 10/10\n",
            "106/106 [==============================] - 1s 7ms/step - loss: nan - accuracy: 0.6042 - val_loss: nan - val_accuracy: 0.4163\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "Updated Model Classification Report for New Testing Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       624\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "    accuracy                           0.42      1499\n",
            "   macro avg       0.21      0.50      0.29      1499\n",
            "weighted avg       0.17      0.42      0.24      1499\n",
            "\n",
            "\n",
            "Updated Model Accuracy for New Testing Set: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assuming the preprocess_data function is already defined as above\n",
        "\n",
        "# Path to the test data and the updated model\n",
        "test_data_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_test.csv'  # Update this path with your test.csv location\n",
        "updated_model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/update_MODEL.h5'  # Ensure this is where your updated model is saved\n",
        "\n",
        "# Preprocess the test data\n",
        "X_test, y_test = preprocess_data(test_data_path)\n",
        "\n",
        "# Load the updated model\n",
        "model = load_model(updated_model_path)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model's performance on the test data\n",
        "print(\"Classification Report for Test Data:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Model Accuracy on Test Data: {accuracy_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuiIa_Z8xp9Y",
        "outputId": "a9771375-0d7e-4335-d048-8c1d11dc1b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 2ms/step\n",
            "Classification Report for Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59       624\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "    accuracy                           0.42      1499\n",
            "   macro avg       0.21      0.50      0.29      1499\n",
            "weighted avg       0.17      0.42      0.24      1499\n",
            "\n",
            "Model Accuracy on Test Data: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ya-zIUzFznCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DloZMsLnznHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QbL6pODsznJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0RbWQtwJznMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Load your dataset\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/Android Dataset/Interview_train.csv'\n",
        "dataset = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract features and labels\n",
        "X = dataset.iloc[:, 2:].values  # Extract features from the 3rd column onwards\n",
        "y = dataset.iloc[:, 1].values  # Extract class labels from the 2nd column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Load pre-trained model\n",
        "pretrained_model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/EDAIC_CNN.h5'  # Update with the path to your pre-trained model\n",
        "pretrained_model = load_model(pretrained_model_path)\n",
        "\n",
        "# Freeze the layers of the pre-trained model\n",
        "for layer in pretrained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add new layers on top of the pre-trained model\n",
        "model = Sequential()\n",
        "model.add(pretrained_model)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks, e.g., to save the best model during training\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=10,  # Adjust as needed\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# Optionally, save the trained model\n",
        "model.save('/content/drive/MyDrive/Depression/EDAIC Dataset/fine-tuned.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93sT4wiWzndh",
        "outputId": "2970fa0d-6063-46a8-e52b-634ccd2c3259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.7888\n",
            "Epoch 1: val_accuracy improved from -inf to 0.86350, saving model to best_model.h5\n",
            "85/85 [==============================] - 23s 242ms/step - loss: 0.6570 - accuracy: 0.7888 - val_loss: 0.6117 - val_accuracy: 0.8635\n",
            "Epoch 2/10\n",
            " 1/85 [..............................] - ETA: 10s - loss: 0.6212 - accuracy: 0.7812"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - ETA: 0s - loss: 0.5726 - accuracy: 0.8471\n",
            "Epoch 2: val_accuracy did not improve from 0.86350\n",
            "85/85 [==============================] - 13s 153ms/step - loss: 0.5726 - accuracy: 0.8471 - val_loss: 0.5196 - val_accuracy: 0.8546\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.8426\n",
            "Epoch 3: val_accuracy improved from 0.86350 to 0.86647, saving model to best_model.h5\n",
            "85/85 [==============================] - 13s 152ms/step - loss: 0.4866 - accuracy: 0.8426 - val_loss: 0.4359 - val_accuracy: 0.8665\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.8497\n",
            "Epoch 4: val_accuracy did not improve from 0.86647\n",
            "85/85 [==============================] - 12s 147ms/step - loss: 0.4176 - accuracy: 0.8497 - val_loss: 0.3844 - val_accuracy: 0.8576\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.8430\n",
            "Epoch 5: val_accuracy did not improve from 0.86647\n",
            "85/85 [==============================] - 12s 146ms/step - loss: 0.3845 - accuracy: 0.8430 - val_loss: 0.3592 - val_accuracy: 0.8591\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.3704 - accuracy: 0.8445\n",
            "Epoch 6: val_accuracy did not improve from 0.86647\n",
            "85/85 [==============================] - 12s 146ms/step - loss: 0.3704 - accuracy: 0.8445 - val_loss: 0.3484 - val_accuracy: 0.8605\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.8471\n",
            "Epoch 7: val_accuracy did not improve from 0.86647\n",
            "85/85 [==============================] - 12s 147ms/step - loss: 0.3563 - accuracy: 0.8471 - val_loss: 0.3435 - val_accuracy: 0.8576\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8422\n",
            "Epoch 8: val_accuracy improved from 0.86647 to 0.86795, saving model to best_model.h5\n",
            "85/85 [==============================] - 14s 160ms/step - loss: 0.3575 - accuracy: 0.8422 - val_loss: 0.3410 - val_accuracy: 0.8680\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.8471\n",
            "Epoch 9: val_accuracy did not improve from 0.86795\n",
            "85/85 [==============================] - 12s 147ms/step - loss: 0.3522 - accuracy: 0.8471 - val_loss: 0.3394 - val_accuracy: 0.8591\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.8474\n",
            "Epoch 10: val_accuracy did not improve from 0.86795\n",
            "85/85 [==============================] - 13s 149ms/step - loss: 0.3515 - accuracy: 0.8474 - val_loss: 0.3391 - val_accuracy: 0.8591\n",
            "22/22 [==============================] - 1s 40ms/step - loss: 0.3391 - accuracy: 0.8591\n",
            "Test accuracy: 0.859050452709198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your dataset\n",
        "csv_file_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/testing_labels.csv'\n",
        "dataset = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract features and labels\n",
        "X = dataset.iloc[:, 2:].values  # Extract features from the 3rd column onwards\n",
        "y_true = dataset.iloc[:, 1].values  # Extract class labels from the 2nd column\n",
        "\n",
        "# Standardize features if needed\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Load your saved CNN model\n",
        "saved_model_path = '/content/drive/MyDrive/Depression/EDAIC Dataset/fine-tuned.h5'  # Update with the path to your saved model\n",
        "model = load_model(saved_model_path)\n",
        "\n",
        "# Make predictions on the dataset\n",
        "predictions = model.predict(X_scaled)\n",
        "predictions_binary = (predictions > 0.5).astype('int32').flatten()  # Assuming binary classification\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, predictions_binary)\n",
        "print(f\"Model accuracy on the dataset: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySLnBNUM1iUu",
        "outputId": "316e0459-2c32-472a-c2a3-0194a44cb116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 3s 39ms/step\n",
            "Model accuracy on the dataset: 66.64%\n"
          ]
        }
      ]
    }
  ]
}